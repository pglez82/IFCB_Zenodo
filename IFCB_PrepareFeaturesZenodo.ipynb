{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/pglez82/IFCB_semisupervised/blob/master/IFCB_FT_Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check that we have the required libraries\n",
    "\n",
    "In this case we are using our IFCB_HDF5 to load the dataset in this format see: https://github.com/pglez82/IFCB_HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "\n",
    "if not os.path.isdir(\"/media/nas/pgonzalez/IFCB_HDF5\"):\n",
    "    print(\"You should have the IFCB_HDF5 project in this directory to run this notebook\")\n",
    "    raise StopExecution\n",
    "sys.path.insert(1, os.path.abspath(\"/media/nas/pgonzalez/IFCB_HDF5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "id": "YoVqmVot04VX",
    "outputId": "910ef3f7-1876-4e8f-91ff-7ff22bc3d551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Sample  roi_number        OriginalClass  \\\n",
      "0        IFCB1_2006_158_000036           1                  mix   \n",
      "1        IFCB1_2006_158_000036           2  Tontonia_gracillima   \n",
      "2        IFCB1_2006_158_000036           3                  mix   \n",
      "3        IFCB1_2006_158_000036           4                  mix   \n",
      "4        IFCB1_2006_158_000036           5                  mix   \n",
      "...                        ...         ...                  ...   \n",
      "3457814  IFCB5_2014_353_205141        6850       Leptocylindrus   \n",
      "3457815  IFCB5_2014_353_205141        6852                  mix   \n",
      "3457816  IFCB5_2014_353_205141        6855                  mix   \n",
      "3457817  IFCB5_2014_353_205141        6856                  mix   \n",
      "3457818  IFCB5_2014_353_205141        6857                  mix   \n",
      "\n",
      "              AutoClass FunctionalGroup  year  \n",
      "0                   mix      Flagellate  2006  \n",
      "1           ciliate_mix         Ciliate  2006  \n",
      "2                   mix      Flagellate  2006  \n",
      "3                   mix      Flagellate  2006  \n",
      "4                   mix      Flagellate  2006  \n",
      "...                 ...             ...   ...  \n",
      "3457814  Leptocylindrus          Diatom  2014  \n",
      "3457815             mix      Flagellate  2014  \n",
      "3457816             mix      Flagellate  2014  \n",
      "3457817             mix      Flagellate  2014  \n",
      "3457818             mix      Flagellate  2014  \n",
      "\n",
      "[3457819 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "    print(\"CSV data do not exist. Downloading...\")\n",
    "    !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "#Compute sample and year information\n",
    "data['year'] = data['Sample'].str[6:10].astype(str) #Compute the year\n",
    "#data[\"day\"] = data[\"Sample\"].str[11:14].astype(str)  # Compute the day\n",
    "samples=data.groupby('Sample').first()\n",
    "#samples = samples[[\"year\", \"day\"]]\n",
    "samples = samples[[\"year\"]]\n",
    "print(data)\n",
    "\n",
    "# Filter by capture device\n",
    "#samples = samples[samples.index.str.contains(\"IFCB1\")]\n",
    "#samples.sort_values([\"year\", \"day\"], ascending=[True, True])  # Sort by year and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Asterionellopsis', 'Cerataulina', 'Ceratium', 'Chaetoceros',\n",
       "        'Corethron', 'Coscinodiscus', 'Cylindrotheca', 'DactFragCerataul',\n",
       "        'Dactyliosolen', 'Dictyocha', 'Dinobryon', 'Dinophysis', 'Ditylum',\n",
       "        'Ephemera', 'Eucampia', 'Euglena', 'Gonyaulax', 'Guinardia',\n",
       "        'Guinardia_flaccida', 'Guinardia_striata', 'Gyrodinium',\n",
       "        'Hemiaulus', 'Laboea', 'Lauderia', 'Leptocylindrus', 'Licmophora',\n",
       "        'Myrionecta', 'Odontella', 'Paralia', 'Phaeocystis', 'Pleurosigma',\n",
       "        'Prorocentrum', 'Pseudonitzschia', 'Pyramimonas', 'Rhizosolenia',\n",
       "        'Skeletonema', 'Stephanopyxis', 'Thalassionema', 'Thalassiosira',\n",
       "        'Thalassiosira_dirty', 'bad', 'ciliate_mix', 'clusterflagellate',\n",
       "        'detritus', 'dino30', 'kiteflagellates', 'mix', 'mix_elongated',\n",
       "        'na', 'pennate', 'tintinnid'], dtype=object),\n",
       " array([   1729,   21556,     328,   40965,    3028,     279,   19516,\n",
       "           5877,    4918,    1074,    3510,     154,    1352,     261,\n",
       "            621,     749,     562,   22570,     641,    2294,    1849,\n",
       "             18,     237,      40,  125129,     143,    1854,      24,\n",
       "            302,     624,     791,    1837,    4214,     614,   21642,\n",
       "          17036,      13,    1829,   12877,    2143,    8848,   11324,\n",
       "            184,  378501,   45807,     229, 2608517,   66224,    7100,\n",
       "           5286,     599]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(data[\"AutoClass\"],return_counts=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oB1gMsg5EIZV"
   },
   "source": [
    "# Create training set\n",
    "\n",
    "Here we make a reestructuration of the images depending on which years we consider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "fX4-tijiEVcO",
    "outputId": "73a7f434-351d-48bc-9340-6313c69c3a35"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(2032)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #AutoClass means 51 classes\n",
    "yearstrainingval = ['2006','2007','2008'] #Years to consider as training and validation\n",
    "yearstest = ['2009','2010','2011','2012','2013','2014'] #Years to consider as test\n",
    "\n",
    "samplestrainingval = list(samples[samples['year'].isin(yearstrainingval)].index)\n",
    "random.shuffle(samplestrainingval) \n",
    "split_index = int(len(samplestrainingval) * 0.7)\n",
    "samplestraining = samplestrainingval[:split_index]\n",
    "samplesval = samplestrainingval[split_index:]\n",
    "\n",
    "samplestest = list(samples[samples['year'].isin(yearstest)].index) #Samples to consider for testing\n",
    "\n",
    "classes=np.unique(data[classcolumn])\n",
    "classes.sort()\n",
    "classes = np.delete(classes, 21) #This class do not exist in the training set, if we find it in validation we will consider it as mix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clYlSmqOJofK"
   },
   "source": [
    "# Configure the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jW90Az7wJqhD",
    "outputId": "136a8a73-3eab-482c-af6a-6d74b924f2c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pytorch 1.10.0\n",
      "Num workers 12. Batch size training 512. Batch size validation 512\n",
      "Output directory already exists, will override everything there...\n",
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch,torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"Using pytorch {}\".format(torch.__version__))\n",
    "\n",
    "torch.manual_seed(0) #Reproducible\n",
    "random.seed(0) #it seems that the transforms uses this random\n",
    "np.random.seed(0)\n",
    "\n",
    "gpus = [0,1] #gpus to use\n",
    "num_gpus = len(gpus)\n",
    "\n",
    "num_workers = 6*num_gpus\n",
    "batch_size = 512 #512 for resnet 18, and 34 (with 2 gpus). 256 for resnet 50\n",
    "batch_size_val = 512\n",
    "\n",
    "print(\"Num workers {}. Batch size training {}. Batch size validation {}\".format(num_workers,batch_size,batch_size_val))\n",
    "\n",
    "num_epochs_ft1 = 10 # @param\n",
    "num_epochs_ft2 = 10 # @param\n",
    "\n",
    "base_model = torchvision.models.resnet34(pretrained=True) #From which model to start\n",
    "\n",
    "is_trained=False #Will take true if the network is already trained\n",
    "model_save_path=\"models/model34_64px_byyears_zenodo.pt\" #Where to save the model once trained\n",
    "results_save_path=\"results_model34_64px_byyears_zenodo\" #Where to save all the predictions for the training and testing samples\n",
    "\n",
    "hdf5_files_path = '/media/nas/pgonzalez/IFCB_HDF5/output/' #Directory with the dataset\n",
    "\n",
    "if not os.path.isdir(results_save_path):\n",
    "    os.mkdir(results_save_path)\n",
    "    os.mkdir(os.path.join(results_save_path,'train'))\n",
    "    os.mkdir(os.path.join(results_save_path,'test'))\n",
    "else:\n",
    "    print(\"Output directory already exists, will override everything there...\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\"%device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uahB4puIqI_"
   },
   "source": [
    "# Prepare de DataLoaders for the CNN\n",
    "In this step it is important to consider that we have to use images with the same size than the original network (so we can reuse the weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbzJMEKsI2Kx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading samples: 100%|██████████| 200/200 [04:58<00:00,  1.49s/it]\n",
      "Loading samples: 100%|██████████| 86/86 [02:10<00:00,  1.52s/it]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as T\n",
    "from ifcb_h5py.h5ifcbdataset import H5IFCBDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=64),\n",
    "  T.RandomResizedCrop(size=64),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor(),            \n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "  T.Resize(size=64),\n",
    "  T.CenterCrop(size=64),\n",
    "  T.ToTensor(),\n",
    "  #T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "#files to load\n",
    "filestraining = [hdf5_files_path+s+'.hdf5' for s in samplestraining]\n",
    "filesval = [hdf5_files_path+s+'.hdf5' for s in samplesval]\n",
    "\n",
    "#Define data loaders\n",
    "train_dset = H5IFCBDataset(filestraining,classes,classattribute=classcolumn, verbose=1,trainingset=True,transform=train_transform)\n",
    "train_loader = DataLoader(train_dset,batch_size=batch_size,num_workers=num_workers,shuffle=True,pin_memory=True)\n",
    "\n",
    "val_dset = H5IFCBDataset(filesval,classes,classattribute=classcolumn, verbose=1,trainingset=False,transform=val_transform)\n",
    "val_loader = DataLoader(val_dset,batch_size=batch_size,num_workers=num_workers,shuffle=False,pin_memory=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "scu2GJIaKXAM"
   },
   "source": [
    "# Define how to load the CNN\n",
    "In this step we download a pretrained CNN with the weights from ImageNet. We change the last layer to match the number of classes that we have in our problem. In the case that model_trained_path is true, that means that we have already trained the network so we load the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "oTq6OVVjKZjm",
    "outputId": "7fd5a702-6dc1-47a5-8d47-f25663febb5d"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def load_network():\n",
    "    global is_trained\n",
    "    global base_model\n",
    "    \n",
    "    model = base_model\n",
    "    print(\"Adjusting the CNN for %s classes\" % len(classes))\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "\n",
    "    print(\"Let's use\", len(gpus), \"GPUs!\")\n",
    "    model = nn.DataParallel(model,device_ids=gpus)\n",
    "\n",
    "    #Define loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    if os.path.isfile(model_save_path):\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        is_trained=True\n",
    "    \n",
    "    model = model.to(device) #Send model to gpu\n",
    "    return model,loss_fn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define finetuning util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "\n",
    "def run_epoch(model, loss_fn, loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    loss_epoch = 0 \n",
    "    start_time = time.time()\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for step, (x, y, _) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Run the model forward to compute scores and loss.\n",
    "        scores = model(x)\n",
    "        loss = loss_fn(scores, y)\n",
    "        loss_epoch = loss_epoch + loss.item()\n",
    "        \n",
    "        # Run the model backward and take a step using the optimizer.\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50== 0 and step!=0:\n",
    "            spent = time.time()-start_time\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss_epoch/step} \\t Time: {spent} secs [{(batch_size*50)/spent} ej/sec]]\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    return loss_epoch\n",
    "\n",
    "#Function for a validation epoch\n",
    "def run_epoch_val(model, loader, device):\n",
    "    with torch.no_grad():\n",
    "        loss_epoch = 0 \n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        for x, y, _ in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # Run the model forward to compute scores and loss.\n",
    "            scores = model(x)\n",
    "            loss = loss_fn(scores, y)\n",
    "            loss_epoch = loss_epoch + loss.item()\n",
    "            #Compute accuracy\n",
    "            _,preds = torch.max(scores,1)\n",
    "            corrects = torch.sum(preds == y)\n",
    "    return corrects.item()/len(y)\n",
    "\n",
    "def make_preds(model, loader, device):\n",
    "    \"\"\"\n",
    "    Check the accuracy of the model.\n",
    "    \"\"\"\n",
    "    modeldeepfeatures =  nn.Sequential(*list(model.module.children())[:-1])\n",
    "    with torch.no_grad():\n",
    "        # Set the model to eval mode\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_logits = []\n",
    "        deepfeatures = []\n",
    "        y_probs = []\n",
    "        sample = []\n",
    "        for x, y, s in loader: #The idea is that the dataloader can give me the sample of the image so we can return it\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Run the model forward, and compare the argmax score with the ground-truth\n",
    "            # category.\n",
    "            output = model(x)\n",
    "            predicted = output.argmax(1)\n",
    "            prob = nnf.softmax(output, dim=1)\n",
    "            y_logits.extend(output.cpu().detach().numpy())\n",
    "            df = modeldeepfeatures(x).view(-1,512).cpu().detach().numpy()\n",
    "            deepfeatures.extend(df)\n",
    "            y_probs.extend(prob.cpu().detach().numpy())\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            sample.extend(s)\n",
    "    return y_true,y_pred,y_probs,y_logits,deepfeatures,sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_W0lTw-LmHt"
   },
   "source": [
    "# Define the finetuning\n",
    "First we only update the last layer for a few epochs, then we update all the weights with a small learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "1s5zIymPLtFc",
    "outputId": "396dcce1-c13f-4739-ad01-26763d62871e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def finetune(model,loss_fn,train_loader,device):\n",
    "    best_val_acc=0\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.module.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.module.fc.parameters(), lr=1e-3)\n",
    "\n",
    "    #First phase of finetuning\n",
    "    for epoch in range(num_epochs_ft1):\n",
    "        # Run an epoch over the training data.\n",
    "        print('Starting epoch %d / %d' % (epoch + 1,num_epochs_ft1))\n",
    "        loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "        # Check accuracy on the train and val sets.\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs_ft1}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "\n",
    "        print(\"Performing validation...\")\n",
    "        val_accuracy = run_epoch_val(model, val_loader, device)\n",
    "        print(\"Validation accuracy %.5f\" % val_accuracy)\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(\"Saved best model in this epoch based on val accuracy\")\n",
    "\n",
    "    #Allow updating all the weights in the second phase\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    #Lower learning rate this time\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # Train the entire model for a few more epochs, checking accuracy on the\n",
    "    # train sets after each epoch.\n",
    "    for epoch in range(num_epochs_ft2):\n",
    "        print('Starting epoch %d / %d' % (epoch + 1, num_epochs_ft2))\n",
    "        loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs_ft2}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "\n",
    "        print(\"Performing validation...\")\n",
    "        val_accuracy = run_epoch_val(model, val_loader, device)\n",
    "        print(\"Validation accuracy %.5f\" % val_accuracy)\n",
    "        if val_accuracy > best_val_acc:\n",
    "            best_val_acc = val_accuracy\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(\"Saved best model in this epoch based on val accuracy\")\n",
    "    \n",
    "    print(\"Fine tune done, reloading best model.\")\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict all validations samples\n",
    "\n",
    "The idea here is to compute the predictions for all the validation samples. We have finetuned the network to the plankton problem and now we iterate over the test samples and compute the output probabilties (file _probs.csv). We also store the _pred (for quantifiers that do not need probability) and the _true label (for checking errors). This is done for each sample so three files will be written per test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process...\n",
      "Adjusting the CNN for 50 classes\n",
      "Let's use 2 GPUs!\n",
      "Starting epoch 1 / 10\n",
      "Step [50/1141]\t Loss: 1.4588942539691925 \t Time: 6.756144285202026 secs [3789.143469903632 ej/sec]]\n",
      "Step [100/1141]\t Loss: 1.1892353880405426 \t Time: 2.404506206512451 secs [10646.676615208577 ej/sec]]\n",
      "Step [150/1141]\t Loss: 1.0715463121732076 \t Time: 2.3832671642303467 secs [10741.556961897419 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.9999886125326156 \t Time: 2.3159477710723877 secs [11053.78986510825 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.9497596781253814 \t Time: 2.397792339324951 secs [10676.487525690882 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.9149046305815379 \t Time: 2.5111584663391113 secs [10194.498014822984 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.8875687289237976 \t Time: 2.343639612197876 secs [10923.181135341965 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.8665475137531757 \t Time: 2.4108335971832275 secs [10618.73371513926 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.848936259481642 \t Time: 2.5936858654022217 secs [9870.123572590015 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.8348857080936432 \t Time: 2.432800054550171 secs [10522.85408828367 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.8224286059899764 \t Time: 2.445347785949707 secs [10468.858518649384 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.812206685145696 \t Time: 2.4743170738220215 secs [10346.28919262003 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.8037535730692057 \t Time: 2.477820634841919 secs [10331.659862713686 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.7958002082790647 \t Time: 2.4780352115631104 secs [10330.765229058983 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.7893632256189982 \t Time: 2.360840082168579 secs [10843.597663965787 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.7831232978403568 \t Time: 2.6758131980895996 secs [9567.185040524188 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.7771311185640447 \t Time: 2.3990683555603027 secs [10670.808916580918 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.7718559860520893 \t Time: 2.4798688888549805 secs [10323.126401984979 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.7672729280120448 \t Time: 2.446965456008911 secs [10461.937636730892 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.7620977482795716 \t Time: 2.562025547027588 secs [9992.093962412131 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.7578191922392163 \t Time: 2.4512293338775635 secs [10443.739247973072 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.7542468945004723 \t Time: 2.4484775066375732 secs [10455.47689558144 ej/sec]]\n",
      "Epoch [1/10]\t Loss: 0.750599291218048\n",
      "Performing validation...\n",
      "Validation accuracy 0.83929\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 2 / 10\n",
      "Step [50/1141]\t Loss: 0.6812891829013824 \t Time: 4.590240240097046 secs [5577.050145736766 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6809563893079758 \t Time: 2.4672622680664062 secs [10375.873019799685 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6819035108884176 \t Time: 2.6734633445739746 secs [9575.594164011045 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6788587054610252 \t Time: 2.413374662399292 secs [10607.55314906198 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6758771095275878 \t Time: 2.4471404552459717 secs [10461.189485516 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.6729306234916052 \t Time: 2.459228038787842 secs [10409.770706997262 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.672344571352005 \t Time: 2.3990590572357178 secs [10670.850274731145 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6716266186535358 \t Time: 2.3952090740203857 secs [10688.002261543752 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6705820147196452 \t Time: 2.489779233932495 secs [10282.036114328877 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6699886453151703 \t Time: 2.5301337242126465 secs [10118.04228172425 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.670781376578591 \t Time: 2.3603296279907227 secs [10845.942743087331 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6704308599233627 \t Time: 2.3913657665252686 secs [10705.179591659717 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.6692451222126301 \t Time: 2.490147113800049 secs [10280.517106049021 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6696195558139256 \t Time: 2.471006155014038 secs [10360.15225945666 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6689655021826426 \t Time: 2.4164960384368896 secs [10593.851424875233 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6688476942479611 \t Time: 2.4627623558044434 secs [10394.831616482925 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6686159693493563 \t Time: 2.381680965423584 secs [10748.710835603886 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6681543781359991 \t Time: 2.386065721511841 secs [10728.958456257242 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6676928328840356 \t Time: 2.4582762718200684 secs [10413.801041591705 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6684893094301224 \t Time: 2.531787872314453 secs [10111.43164083394 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6682984770479656 \t Time: 2.367281913757324 secs [10814.09013908612 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.6681481267105449 \t Time: 2.4486443996429443 secs [10454.764278444405 ej/sec]]\n",
      "Epoch [2/10]\t Loss: 0.667262808206727\n",
      "Performing validation...\n",
      "Validation accuracy 0.81696\n",
      "Starting epoch 3 / 10\n",
      "Step [50/1141]\t Loss: 0.6928788006305695 \t Time: 4.634486675262451 secs [5523.804855593909 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6769238477945327 \t Time: 2.4349355697631836 secs [10513.625213701158 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6739933470884959 \t Time: 2.4184422492980957 secs [10585.326156715086 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6708970379829406 \t Time: 2.3296611309051514 secs [10988.722634546228 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6680123963356018 \t Time: 2.471775770187378 secs [10356.926509583569 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.670566454132398 \t Time: 2.43886399269104 secs [10496.690293808875 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.6704797279834748 \t Time: 2.3770718574523926 secs [10769.552430542251 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6699303032457828 \t Time: 2.510244369506836 secs [10198.210306126248 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6689220339722104 \t Time: 2.300980806350708 secs [11125.690370534161 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6672958180904388 \t Time: 2.3479013442993164 secs [10903.354207005577 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6668094024333087 \t Time: 2.4129223823547363 secs [10609.541437059126 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6658173416554928 \t Time: 2.4920454025268555 secs [10272.686032944026 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.6650315776696571 \t Time: 2.5144248008728027 secs [10181.254969770333 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6640713137813977 \t Time: 2.369504928588867 secs [10803.944609326389 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6633192556301752 \t Time: 2.464078664779663 secs [10389.278705227189 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6626798303797841 \t Time: 2.4441092014312744 secs [10474.163750542977 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6625156414859436 \t Time: 2.4972825050354004 secs [10251.142971762862 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6623080763220787 \t Time: 2.4520180225372314 secs [10440.380031754554 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6624883611892399 \t Time: 2.5248446464538574 secs [10139.237689714962 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6628398293554782 \t Time: 2.3909106254577637 secs [10707.217462425482 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6628309065671194 \t Time: 2.4217727184295654 secs [10570.769009488513 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.6626490603522821 \t Time: 2.54770565032959 secs [10048.256554554564 ej/sec]]\n",
      "Epoch [3/10]\t Loss: 0.6622455314154378\n",
      "Performing validation...\n",
      "Validation accuracy 0.81696\n",
      "Starting epoch 4 / 10\n",
      "Step [50/1141]\t Loss: 0.6905159258842468 \t Time: 4.865029335021973 secs [5262.04432431945 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6714113247394562 \t Time: 2.359492063522339 secs [10849.792798956634 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6741139296690623 \t Time: 2.441187620162964 secs [10486.69909209643 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6697803273797035 \t Time: 2.423189163208008 secs [10564.589999283718 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6689991314411163 \t Time: 2.4321515560150146 secs [10525.659857293022 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.669783576130867 \t Time: 2.359726905822754 secs [10848.713017099824 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.667171699489866 \t Time: 2.5691909790039062 secs [9964.226174391015 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6663347315788269 \t Time: 2.449578285217285 secs [10450.778468477973 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6653292409578959 \t Time: 2.4790732860565186 secs [10326.439377160214 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6654658389091491 \t Time: 2.509047269821167 secs [10203.076007342279 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6643332947384227 \t Time: 2.559252977371216 secs [10002.918908897984 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6644250556826592 \t Time: 2.409402847290039 secs [10625.0393240771 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.663602824486219 \t Time: 2.418957471847534 secs [10583.07154959918 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6627563304560525 \t Time: 2.2092392444610596 secs [11587.699278918559 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6624262529214223 \t Time: 2.442800998687744 secs [10479.773020296023 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6628659753501416 \t Time: 2.3608100414276123 secs [10843.735646142604 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6626640957944533 \t Time: 2.5057530403137207 secs [10216.489649273208 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6620684747563468 \t Time: 2.459651470184326 secs [10407.978654830124 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6623248333680002 \t Time: 2.4410343170166016 secs [10487.35768339708 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6624022865891457 \t Time: 2.526634454727173 secs [10132.055292804238 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6620599542061488 \t Time: 2.5257461071014404 secs [10135.618908021874 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.6609217672185465 \t Time: 2.511078119277954 secs [10194.82420855992 ej/sec]]\n",
      "Epoch [4/10]\t Loss: 0.6598344376011964\n",
      "Performing validation...\n",
      "Validation accuracy 0.85714\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 5 / 10\n",
      "Step [50/1141]\t Loss: 0.6800818693637848 \t Time: 4.488527297973633 secs [5703.4297221623765 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.668458279967308 \t Time: 2.3875067234039307 secs [10722.482893577537 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6642185695966085 \t Time: 2.3986759185791016 secs [10672.554721425067 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.660903694331646 \t Time: 2.427633285522461 secs [10545.250039480537 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6609864819049835 \t Time: 2.5792765617370605 secs [9925.263688186744 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.6604380359252294 \t Time: 2.393083333969116 secs [10697.496253730702 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.6588324325425284 \t Time: 2.5877561569213867 secs [9892.74044678766 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6588623584806919 \t Time: 2.5135302543640137 secs [10184.878401822716 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6580210977130466 \t Time: 2.3553872108459473 secs [10868.70128279488 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6576233983039856 \t Time: 2.6529958248138428 secs [9649.468634876694 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6582580099322579 \t Time: 2.4567983150482178 secs [10420.065759243069 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6578361834088962 \t Time: 2.321593761444092 secs [11026.907646442043 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.6578431999683381 \t Time: 2.383491277694702 secs [10740.546961329836 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6585088695798601 \t Time: 2.5025904178619385 secs [10229.400631155253 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6585259665648142 \t Time: 2.359659194946289 secs [10849.024323015727 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6589862516522408 \t Time: 2.378096580505371 secs [10764.91182480054 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6587125991372501 \t Time: 2.684260368347168 secs [9537.077811778441 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6581728723976347 \t Time: 2.4787566661834717 secs [10327.758407773112 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6574513387052636 \t Time: 2.4515769481658936 secs [10442.258408063517 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6571791233420372 \t Time: 2.4709253311157227 secs [10360.49113974665 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6569383538904644 \t Time: 2.457557439804077 secs [10416.847063416308 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.656963534842838 \t Time: 2.4291975498199463 secs [10538.459501532714 ej/sec]]\n",
      "Epoch [5/10]\t Loss: 0.6563935721787729\n",
      "Performing validation...\n",
      "Validation accuracy 0.85714\n",
      "Starting epoch 6 / 10\n",
      "Step [50/1141]\t Loss: 0.6513590490818024 \t Time: 4.430922269821167 secs [5777.578219857425 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6574489343166351 \t Time: 2.4345831871032715 secs [10515.146960519154 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6596257627010346 \t Time: 2.445221424102783 secs [10469.399518447832 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6601700600981713 \t Time: 2.4104413986206055 secs [10620.461470106598 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6606972122192383 \t Time: 2.443042278289795 secs [10478.73801755113 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.6580066529909769 \t Time: 2.5516083240509033 secs [10032.887790300723 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.6581386629172734 \t Time: 2.3912582397460938 secs [10705.660967306581 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6591221062839031 \t Time: 2.4221155643463135 secs [10569.272736955056 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6591276048289405 \t Time: 2.538112163543701 secs [10086.23667925589 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6579360699653626 \t Time: 2.482452869415283 secs [10312.38107897284 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6580392346598886 \t Time: 2.461693048477173 secs [10399.346911198538 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6581121840079626 \t Time: 2.3895578384399414 secs [10713.279079577895 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.6593034256421603 \t Time: 2.4650042057037354 secs [10385.377818327674 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6593168362549373 \t Time: 2.3869006633758545 secs [10725.20544855573 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6584005630016327 \t Time: 2.4635062217712402 secs [10391.692853770759 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6594451653957367 \t Time: 2.386863946914673 secs [10725.370431394414 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6584952691022088 \t Time: 2.5162084102630615 secs [10174.038007179064 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6577115432421367 \t Time: 2.3579609394073486 secs [10856.838029909995 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6579063079859081 \t Time: 2.4560561180114746 secs [10423.214605017587 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6579146323800087 \t Time: 2.588844060897827 secs [9888.583243257131 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6578254277365548 \t Time: 2.450382947921753 secs [10447.346616459345 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.6582545054500754 \t Time: 2.3393642902374268 secs [10943.143873245072 ej/sec]]\n",
      "Epoch [6/10]\t Loss: 0.6570860341543905\n",
      "Performing validation...\n",
      "Validation accuracy 0.84821\n",
      "Starting epoch 7 / 10\n",
      "Step [50/1141]\t Loss: 0.6759800004959107 \t Time: 4.432531118392944 secs [5775.4811678076885 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6663417559862137 \t Time: 2.425400972366333 secs [10554.95577501293 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6647184948126476 \t Time: 2.4060051441192627 secs [10640.043751598496 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6637928053736687 \t Time: 2.5284571647644043 secs [10124.751313469591 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6627394480705261 \t Time: 2.4830050468444824 secs [10310.08778356438 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.6621020958820979 \t Time: 2.5946576595306396 secs [9866.426850558355 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.6623065888881683 \t Time: 2.428276777267456 secs [10542.455555172637 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6603652063012123 \t Time: 2.444701910018921 secs [10471.624329774368 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6595881859461467 \t Time: 2.494661331176758 secs [10261.913984101486 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6600521982908248 \t Time: 2.4556772708892822 secs [10424.822635887082 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6597357507185503 \t Time: 2.3630990982055664 secs [10833.231674219467 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6600074432293574 \t Time: 2.4521372318267822 secs [10439.872478478143 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.6594334997580602 \t Time: 2.374131679534912 secs [10782.88968580504 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6586251464060374 \t Time: 2.3702890872955322 secs [10800.370358709812 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6578673811753591 \t Time: 2.3508148193359375 secs [10889.841168872474 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6574627950787544 \t Time: 2.5624351501464844 secs [9990.4967345365 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6579500586846295 \t Time: 2.3179686069488525 secs [11044.153024012408 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6573631085289849 \t Time: 2.3791518211364746 secs [10760.137193670716 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6574716670889603 \t Time: 2.462311267852783 secs [10396.73591808888 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6572043506503105 \t Time: 2.496614456176758 secs [10253.885992153986 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.656797684771674 \t Time: 2.436183214187622 secs [10508.240862556251 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.657235382903706 \t Time: 2.383234739303589 secs [10741.703105368731 ej/sec]]\n",
      "Epoch [7/10]\t Loss: 0.6562860483249795\n",
      "Performing validation...\n",
      "Validation accuracy 0.85268\n",
      "Starting epoch 8 / 10\n",
      "Step [50/1141]\t Loss: 0.669879845380783 \t Time: 4.3271262645721436 secs [5916.166627629312 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6635110431909561 \t Time: 2.406449556350708 secs [10638.078796391417 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6613480472564697 \t Time: 2.4369163513183594 secs [10505.079497763856 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6585274338722229 \t Time: 2.416400909423828 secs [10594.268484240936 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6578780009746551 \t Time: 2.493522882461548 secs [10266.599187863989 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.6575061277548472 \t Time: 2.415900468826294 secs [10596.463029140075 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.6577457182747977 \t Time: 2.4090416431427 secs [10626.63240914494 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6564836683869362 \t Time: 2.348562479019165 secs [10900.28484602691 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6572986800140804 \t Time: 2.4814293384552 secs [10316.634692462021 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6571976773738861 \t Time: 2.539525032043457 secs [10080.625186592737 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6574385443600741 \t Time: 2.4638636112213135 secs [10390.18551327617 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6565237462520599 \t Time: 2.437152147293091 secs [10504.063124837547 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.6564209117339208 \t Time: 2.336229085922241 secs [10957.829501508084 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6567337501049042 \t Time: 2.385854721069336 secs [10729.90730488658 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6560984840393066 \t Time: 2.416644334793091 secs [10593.201337668843 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.655534651055932 \t Time: 2.6529884338378906 secs [9649.495517387648 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6554354980412651 \t Time: 2.455191135406494 secs [10426.886783200092 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6558824945820703 \t Time: 2.367295980453491 secs [10814.025880741763 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6562054504846272 \t Time: 2.5540122985839844 secs [10023.444293589875 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6560209134817123 \t Time: 2.4527969360351562 secs [10437.064570612734 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6561318417957851 \t Time: 2.3829824924468994 secs [10742.840151424423 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.6561847288500179 \t Time: 2.4003546237945557 secs [10665.090793763931 ej/sec]]\n",
      "Epoch [8/10]\t Loss: 0.6555455648825944\n",
      "Performing validation...\n",
      "Validation accuracy 0.85714\n",
      "Starting epoch 9 / 10\n",
      "Step [50/1141]\t Loss: 0.6781081223487854 \t Time: 4.416829586029053 secs [5796.012615242342 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6623763656616211 \t Time: 2.481003999710083 secs [10318.403357266445 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6610199320316315 \t Time: 2.434500217437744 secs [10515.505324925958 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6578777959942818 \t Time: 2.548741579055786 secs [10044.172469412864 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6571419954299926 \t Time: 2.593538761138916 secs [9870.68340122209 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.6571579029162725 \t Time: 2.464421510696411 secs [10387.833367338933 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.6563096444947379 \t Time: 2.375761032104492 secs [10775.49452746224 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6562566366791726 \t Time: 2.5378849506378174 secs [10087.139684392016 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6557218705283271 \t Time: 2.402135133743286 secs [10657.185618074327 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6549656090736389 \t Time: 2.386129856109619 secs [10728.6700824148 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6541919863224029 \t Time: 2.4504261016845703 secs [10447.162631185254 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.6540339081486066 \t Time: 2.363600254058838 secs [10830.93469635527 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.6550277883272905 \t Time: 2.3048183917999268 secs [11107.16579279286 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6552615519932338 \t Time: 2.3439300060272217 secs [10921.827842201654 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6561862658659617 \t Time: 2.48193097114563 secs [10314.549557429207 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6552458454668522 \t Time: 2.464857339859009 secs [10385.996619773676 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.6548679770441617 \t Time: 2.4953453540802 secs [10259.10099303121 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6550844841533237 \t Time: 2.3962912559509277 secs [10683.17548479351 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.6544051270108474 \t Time: 2.476219654083252 secs [10338.339717878402 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6542974448204041 \t Time: 2.3626797199249268 secs [10835.154584902193 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6544325372150966 \t Time: 2.446160078048706 secs [10465.382143110208 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.6547082007473165 \t Time: 2.37658429145813 secs [10771.761848300937 ej/sec]]\n",
      "Epoch [9/10]\t Loss: 0.6547452697724653\n",
      "Performing validation...\n",
      "Validation accuracy 0.84375\n",
      "Starting epoch 10 / 10\n",
      "Step [50/1141]\t Loss: 0.6787940621376037 \t Time: 4.426852226257324 secs [5782.890119565496 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6631339991092682 \t Time: 2.395237684249878 secs [10687.874597304197 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.6597169391314188 \t Time: 2.415419101715088 secs [10598.57479052911 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.6604195696115494 \t Time: 2.529005765914917 secs [10122.55501550377 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.6589178624153137 \t Time: 2.414414405822754 secs [10602.985112357443 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.6565379267930984 \t Time: 2.439255952835083 secs [10495.00359740674 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.6563838246890477 \t Time: 2.521667957305908 secs [10152.010666523458 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.6553394176065922 \t Time: 2.5599701404571533 secs [10000.116640199723 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.6539187716113196 \t Time: 2.484489679336548 secs [10303.9268840256 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.6554133257865906 \t Time: 2.2657201290130615 secs [11298.835929551129 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.6549452248486606 \t Time: 2.3537790775299072 secs [10876.126924734603 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.654567334651947 \t Time: 2.4130795001983643 secs [10608.850639979157 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.654956800204057 \t Time: 2.457958698272705 secs [10415.14652707144 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.6546620715515954 \t Time: 2.4505269527435303 secs [10446.732679817731 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.6536259406407674 \t Time: 2.5662944316864014 secs [9975.472683069085 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.6536939887702465 \t Time: 2.398230791091919 secs [10674.535618127175 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.653640222829931 \t Time: 2.401594400405884 secs [10659.585147131193 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.6538838343487845 \t Time: 2.4591217041015625 secs [10410.220835065556 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.653870937196832 \t Time: 2.4677035808563232 secs [10374.017446259282 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.6536909517645836 \t Time: 2.3926000595092773 secs [10699.657010478619 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.6539966836997441 \t Time: 2.3949856758117676 secs [10688.999211372326 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.6529621977155858 \t Time: 2.4562809467315674 secs [10422.260545588018 ej/sec]]\n",
      "Epoch [10/10]\t Loss: 0.6521099482995183\n",
      "Performing validation...\n",
      "Validation accuracy 0.85268\n",
      "Starting epoch 1 / 10\n",
      "Step [50/1141]\t Loss: 0.6335614067316055 \t Time: 7.247903347015381 secs [3532.0559304287412 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.6098798486590385 \t Time: 5.183332920074463 secs [4938.907146182737 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.596708043217659 \t Time: 5.451389312744141 secs [4696.050590287667 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.5900321155786514 \t Time: 5.2912633419036865 secs [4838.164034903175 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.5819235008955002 \t Time: 5.340113162994385 secs [4793.905900234744 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.5746257559458414 \t Time: 5.309705018997192 secs [4821.360114810087 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.5686037818023136 \t Time: 5.360277414321899 secs [4775.872221016106 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.563923642784357 \t Time: 5.380489826202393 secs [4757.9311228005345 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.5591440622674094 \t Time: 5.22872257232666 secs [4896.033332403137 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.5547016471028328 \t Time: 5.426364183425903 secs [4717.707683201902 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.5505179805105382 \t Time: 5.3492395877838135 secs [4785.726939294948 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.5476841009656588 \t Time: 5.363969564437866 secs [4772.584872539789 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.5435373322321818 \t Time: 5.336541414260864 secs [4797.114462859597 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.53940696648189 \t Time: 5.498162746429443 secs [4656.100806878601 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.5362772740523021 \t Time: 5.353363513946533 secs [4782.040287999706 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.5333151023462415 \t Time: 5.330080032348633 secs [4802.929758020853 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.53095787051846 \t Time: 5.312681436538696 secs [4818.658958907734 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.528302083114783 \t Time: 5.483605146408081 secs [4668.461589866429 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.5259789942126525 \t Time: 5.2690346240997314 secs [4858.575019209335 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.5230885713994503 \t Time: 5.430757761001587 secs [4713.890975553038 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.5205650889873504 \t Time: 5.414107084274292 secs [4728.388190613601 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.5182629641348665 \t Time: 5.366968870162964 secs [4769.917735562099 ej/sec]]\n",
      "Epoch [1/10]\t Loss: 0.5158343458520436\n",
      "Performing validation...\n",
      "Validation accuracy 0.90625\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 2 / 10\n",
      "Step [50/1141]\t Loss: 0.45725256860256197 \t Time: 7.327336311340332 secs [3493.7662081075127 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.45576546669006346 \t Time: 5.354325532913208 secs [4781.181092303034 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.45715055624643963 \t Time: 5.288175106048584 secs [4840.9894692630105 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.4580699160695076 \t Time: 5.339408874511719 secs [4794.538234785603 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.4570572634935379 \t Time: 5.395374298095703 secs [4744.805195264305 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.45738362620274226 \t Time: 5.247766733169556 secs [4878.265613101683 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.455282393012728 \t Time: 5.353302001953125 secs [4782.0952359235425 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.45488354459404945 \t Time: 5.439659833908081 secs [4706.1766326678335 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.45526880058977337 \t Time: 5.4383556842803955 secs [4707.3052014595105 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.45346523118019105 \t Time: 5.281798839569092 secs [4846.833584084118 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.45177277516235004 \t Time: 5.389588117599487 secs [4749.89914654224 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.4514082227150599 \t Time: 5.4385387897491455 secs [4707.1467152633495 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.45064359490688033 \t Time: 5.465265274047852 secs [4684.127616195169 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.45029356343405585 \t Time: 5.325088024139404 secs [4807.432268528041 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.4491933099826177 \t Time: 5.231998920440674 secs [4892.967370460351 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.448680895678699 \t Time: 5.307116746902466 secs [4823.711484195559 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.44814893515671 \t Time: 5.325672388076782 secs [4806.904768929041 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.44774290558364654 \t Time: 5.341570615768433 secs [4792.597878314712 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.44730257658581984 \t Time: 5.45990252494812 secs [4688.728394513462 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.446941772967577 \t Time: 5.4630725383758545 secs [4686.007703571653 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.44626774512586137 \t Time: 5.232096195220947 secs [4892.87640074036 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.44584564628926193 \t Time: 5.302053928375244 secs [4828.317543696663 ej/sec]]\n",
      "Epoch [2/10]\t Loss: 0.44475325330843746\n",
      "Performing validation...\n",
      "Validation accuracy 0.91964\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 3 / 10\n",
      "Step [50/1141]\t Loss: 0.436478915810585 \t Time: 7.6596550941467285 secs [3342.1870417589334 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.4326982614398003 \t Time: 5.414285898208618 secs [4728.232029355906 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.42975526571273803 \t Time: 5.262018918991089 secs [4865.052823661912 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.4297207772731781 \t Time: 5.268587350845337 secs [4858.98748473678 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.42825920498371123 \t Time: 5.508834600448608 secs [4647.08089037839 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.4278054505586624 \t Time: 5.328509330749512 secs [4804.345532862018 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.4269147365433829 \t Time: 5.241790533065796 secs [4883.827356036522 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.42431914508342744 \t Time: 5.215817451477051 secs [4908.147234476241 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.4230935629208883 \t Time: 5.326651334762573 secs [4806.021342701808 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.42199403113126754 \t Time: 5.177694082260132 secs [4944.285929852631 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.42061807052655653 \t Time: 5.242003440856934 secs [4883.628995828178 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.42015518377224603 \t Time: 5.423604965209961 secs [4720.107781487172 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.41942197955571686 \t Time: 5.262669563293457 secs [4864.451338263225 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.4196916606596538 \t Time: 5.311327695846558 secs [4819.88712916718 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.4194843210776647 \t Time: 5.414819955825806 secs [4727.765689135602 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.41918939862400295 \t Time: 5.301611661911011 secs [4828.720327428181 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.418849725372651 \t Time: 5.244951486587524 secs [4880.884039721766 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.41892811175849703 \t Time: 5.221353054046631 secs [4902.943688161366 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.4184532650520927 \t Time: 5.266972303390503 secs [4860.477429038413 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.4181359500288963 \t Time: 5.282039403915405 secs [4846.61284068111 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.41809981229759396 \t Time: 5.207408428192139 secs [4916.073005029793 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.4176340620897033 \t Time: 5.375872611999512 secs [4762.017601171969 ej/sec]]\n",
      "Epoch [3/10]\t Loss: 0.4171727413571163\n",
      "Performing validation...\n",
      "Validation accuracy 0.91518\n",
      "Starting epoch 4 / 10\n",
      "Step [50/1141]\t Loss: 0.4129579973220825 \t Time: 7.4671971797943115 secs [3428.327842911625 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.40722989350557326 \t Time: 5.355835914611816 secs [4779.832767123795 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.40462848405043284 \t Time: 5.323573112487793 secs [4808.800303681131 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.40697664842009545 \t Time: 5.386915445327759 secs [4752.255768596421 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.4064328179359436 \t Time: 5.371701002120972 secs [4765.715736950372 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.4073109990358353 \t Time: 5.470792293548584 secs [4679.395346481848 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.4059896924665996 \t Time: 5.329291820526123 secs [4803.640119949876 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.40601354129612444 \t Time: 5.273578643798828 secs [4854.388590583151 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.4060047894053989 \t Time: 5.403502941131592 secs [4737.6674499669825 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.4049980166554451 \t Time: 5.350279808044434 secs [4784.796481393183 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.4043720216100866 \t Time: 5.208601236343384 secs [4914.947188003986 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.40381927117705346 \t Time: 5.399444818496704 secs [4741.228192999196 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.40304893975074474 \t Time: 5.311835527420044 secs [4819.4263297218295 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.4026978461231504 \t Time: 5.307441473007202 secs [4823.416354225949 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.4025641528765361 \t Time: 5.3922810554504395 secs [4747.527018111175 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.4021267769113183 \t Time: 5.4616265296936035 secs [4687.248361054844 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.4018106955991072 \t Time: 5.484536409378052 secs [4667.668894717584 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.4017110605041186 \t Time: 5.356727361679077 secs [4779.037324754872 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.4010034542962124 \t Time: 5.325429439544678 secs [4807.12406212799 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.40110114952921866 \t Time: 5.416842460632324 secs [4726.000467255907 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.40059391396386285 \t Time: 5.316619634628296 secs [4815.089616955415 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.40013400546529077 \t Time: 5.331517934799194 secs [4801.634415014717 ej/sec]]\n",
      "Epoch [4/10]\t Loss: 0.3998180185097754\n",
      "Performing validation...\n",
      "Validation accuracy 0.91518\n",
      "Starting epoch 5 / 10\n",
      "Step [50/1141]\t Loss: 0.4031061798334122 \t Time: 7.429068088531494 secs [3445.923458356721 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.39458402156829836 \t Time: 5.334585189819336 secs [4798.8735935562 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.3931219619512558 \t Time: 5.459048748016357 secs [4689.461695923162 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.39613102003932 \t Time: 5.348908424377441 secs [4786.023234820958 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.3953423653841019 \t Time: 5.3642966747283936 secs [4772.293844336301 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.39454291105270384 \t Time: 5.279456615447998 secs [4848.983875555092 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.39180372859750473 \t Time: 5.365311622619629 secs [4771.391076721975 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.3922482904791832 \t Time: 5.333250045776367 secs [4800.074959971876 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.39133542716503145 \t Time: 5.277361631393433 secs [4850.908804072345 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.39002795016765596 \t Time: 5.431403875350952 secs [4713.330215817517 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.389735312407667 \t Time: 5.384106636047363 secs [4754.734950568093 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.38972993021210034 \t Time: 5.321354627609253 secs [4810.805103493247 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.3899621550853436 \t Time: 5.482881307601929 secs [4669.0779106427135 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.38978876833404813 \t Time: 5.486147165298462 secs [4666.298447465597 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.38914033861955005 \t Time: 5.311591386795044 secs [4819.647848598301 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.3896859822794795 \t Time: 5.309166431427002 secs [4821.849216943688 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.3889012812165653 \t Time: 5.346814393997192 secs [4787.897636533041 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.38876309788889357 \t Time: 5.383455038070679 secs [4755.31045006638 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.3884313083949842 \t Time: 5.34787654876709 secs [4786.946700537033 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.3881322067081928 \t Time: 5.279961109161377 secs [4848.5205611800575 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.38795705383732204 \t Time: 5.441680908203125 secs [4704.428729256981 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.38779089865359395 \t Time: 5.286372184753418 secs [4842.640492440868 ej/sec]]\n",
      "Epoch [5/10]\t Loss: 0.3874180993158289\n",
      "Performing validation...\n",
      "Validation accuracy 0.92857\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 6 / 10\n",
      "Step [50/1141]\t Loss: 0.39457454681396487 \t Time: 8.218943357467651 secs [3114.75561840198 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.3934312844276428 \t Time: 5.237746953964233 secs [4887.597706610171 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.3883845372994741 \t Time: 5.454942941665649 secs [4692.991342670785 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.3874859561026096 \t Time: 5.2792980670928955 secs [4849.129500675632 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.38325844824314115 \t Time: 5.308941841125488 secs [4822.053201203054 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.3830839785933495 \t Time: 5.163054943084717 secs [4958.304779283451 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.38105415446417673 \t Time: 5.2701709270477295 secs [4857.527460563928 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.38132885232567787 \t Time: 5.470425605773926 secs [4679.709010754064 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.38170858561992643 \t Time: 5.37242579460144 secs [4765.072795556252 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.38093092918396 \t Time: 5.412998199462891 secs [4729.356829001012 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.380620603073727 \t Time: 5.247467041015625 secs [4878.544219506947 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.3805826435983181 \t Time: 5.2059385776519775 secs [4917.461014598891 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.38017345625620624 \t Time: 5.364866256713867 secs [4771.787175116035 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.3796561132158552 \t Time: 5.288328170776367 secs [4840.84935225223 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.3794373360474904 \t Time: 5.301631450653076 secs [4828.702303862802 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.37919946007430555 \t Time: 5.3352086544036865 secs [4798.312804292993 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.37943966308060817 \t Time: 5.427859783172607 secs [4716.407759714952 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.37895827104647956 \t Time: 5.323225021362305 secs [4809.114756048491 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.3786834953646911 \t Time: 5.312822341918945 secs [4818.531159608379 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.3779822343587875 \t Time: 5.281685829162598 secs [4846.937290107397 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.377926841378212 \t Time: 5.1737220287323 secs [4948.081836988967 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.3775891364704479 \t Time: 5.296681642532349 secs [4833.214780067585 ej/sec]]\n",
      "Epoch [6/10]\t Loss: 0.3771410380195464\n",
      "Performing validation...\n",
      "Validation accuracy 0.93304\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 7 / 10\n",
      "Step [50/1141]\t Loss: 0.38363085269927977 \t Time: 7.410402297973633 secs [3454.6032685702226 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.3807788750529289 \t Time: 5.3662109375 secs [4770.59144676979 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.37714834392070773 \t Time: 5.22870659828186 secs [4896.048290109086 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.3761251229047775 \t Time: 5.353907585144043 secs [4781.554330716235 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.3753147360086441 \t Time: 5.4146411418914795 secs [4727.921819590289 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.37414112508296965 \t Time: 5.219064950942993 secs [4905.093199764554 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.3739431015934263 \t Time: 5.376285791397095 secs [4761.651629636958 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.3738047003000975 \t Time: 5.447671413421631 secs [4699.2555272200025 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.372959964076678 \t Time: 5.319992542266846 secs [4812.036820843335 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.37163308811187745 \t Time: 5.318840742111206 secs [4813.078872115017 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.3717002715305849 \t Time: 5.399027109146118 secs [4741.595010077429 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.370932967265447 \t Time: 5.466937065124512 secs [4682.695208494585 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.3709716260433197 \t Time: 5.294966220855713 secs [4834.780607129693 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.37126321669135776 \t Time: 5.145271301269531 secs [4975.442207232789 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.3707606422503789 \t Time: 5.234457015991211 secs [4890.66963809088 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.37040159370750186 \t Time: 5.318575859069824 secs [4813.318579699121 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.3704824582969441 \t Time: 5.223440647125244 secs [4900.984184454959 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.37106857584582437 \t Time: 5.296003103256226 secs [4833.834025561644 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.37084782700789604 \t Time: 5.438765287399292 secs [4706.950685904926 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.3708610387742519 \t Time: 5.322473049163818 secs [4809.794199713583 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.3705337769076938 \t Time: 5.31131911277771 secs [4819.8949180840555 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.36997030130841513 \t Time: 5.238918781280518 secs [4886.50446185057 ej/sec]]\n",
      "Epoch [7/10]\t Loss: 0.3694172091227054\n",
      "Performing validation...\n",
      "Validation accuracy 0.93750\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 8 / 10\n",
      "Step [50/1141]\t Loss: 0.37358207404613497 \t Time: 7.509858846664429 secs [3408.852352980039 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.37193913131952283 \t Time: 5.360464334487915 secs [4775.70568566157 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.37067570666472116 \t Time: 5.318923711776733 secs [4813.003793101701 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.3686644360423088 \t Time: 5.1717140674591064 secs [4950.00297117691 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.3680498801469803 \t Time: 5.327677965164185 secs [4805.095234244526 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.36739388714234034 \t Time: 5.2361180782318115 secs [4889.118163019899 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.3676399341651371 \t Time: 5.3358118534088135 secs [4797.770368092214 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.3668061513453722 \t Time: 5.4246742725372314 secs [4719.177357726652 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.36584659841325545 \t Time: 5.215842008590698 secs [4908.124126044421 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.3669031324386597 \t Time: 5.498203277587891 secs [4656.066483455108 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.36648603211749686 \t Time: 5.365565776824951 secs [4771.165067171851 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.36609429761767387 \t Time: 5.393331050872803 secs [4746.60275042029 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.36557811700380766 \t Time: 5.298669099807739 secs [4831.401908250676 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.3655011141300201 \t Time: 5.324003219604492 secs [4808.411817959374 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.36461940248807273 \t Time: 5.356933832168579 secs [4778.853127935067 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.36397576950490473 \t Time: 5.364098310470581 secs [4772.470323675736 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.3642278913890614 \t Time: 5.337115287780762 secs [4796.598652948491 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.36391081276867127 \t Time: 5.228892087936401 secs [4895.874607751394 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.36387130555353664 \t Time: 5.293426990509033 secs [4836.186471618497 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.3640327882170677 \t Time: 5.415907382965088 secs [4726.816429786245 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.36381898990699224 \t Time: 5.459132194519043 secs [4689.390014351062 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.3641781461238861 \t Time: 5.453649282455444 secs [4694.104566341656 ej/sec]]\n",
      "Epoch [8/10]\t Loss: 0.3638318114955628\n",
      "Performing validation...\n",
      "Validation accuracy 0.93750\n",
      "Starting epoch 9 / 10\n",
      "Step [50/1141]\t Loss: 0.3620981457829475 \t Time: 7.305986166000366 secs [3503.9759750892904 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.3622968973219395 \t Time: 5.27920389175415 secs [4849.216003948229 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.36066713561614355 \t Time: 5.361055612564087 secs [4775.178966620722 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.3622123358398676 \t Time: 5.2007575035095215 secs [4922.359864447606 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.3610296211838722 \t Time: 5.1087400913238525 secs [5011.020240289059 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.36036996578176816 \t Time: 5.3518736362457275 secs [4783.371533031576 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.36037952427353176 \t Time: 5.288799524307251 secs [4840.417921371901 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.35975655529648065 \t Time: 5.258211612701416 secs [4868.575455990055 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.3598504531052377 \t Time: 5.420171499252319 secs [4723.097784550058 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.35965563836693765 \t Time: 5.277100086212158 secs [4851.149226236372 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.3593298544937914 \t Time: 5.3011157512664795 secs [4829.17204625912 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.359086026524504 \t Time: 5.316342830657959 secs [4815.340322368132 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.35855679555581166 \t Time: 5.297108173370361 secs [4832.825602598867 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.35888728633522987 \t Time: 5.267766952514648 secs [4859.74422003985 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.3589205061395963 \t Time: 5.261713743209839 secs [4865.334993382415 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.3585505429469049 \t Time: 5.400542974472046 secs [4740.2641032594765 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.3584092279216822 \t Time: 5.265931844711304 secs [4861.437776812601 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.35820046231150626 \t Time: 5.303393602371216 secs [4827.097877207136 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.3579771954291745 \t Time: 5.16811990737915 secs [4953.44544220961 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.3578857755810022 \t Time: 5.379487037658691 secs [4758.818047295056 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.3576658169570423 \t Time: 5.314486026763916 secs [4817.022732034219 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.35765263496474786 \t Time: 5.2756102085113525 secs [4852.519232504801 ej/sec]]\n",
      "Epoch [9/10]\t Loss: 0.3572358163979678\n",
      "Performing validation...\n",
      "Validation accuracy 0.94196\n",
      "Saved best model in this epoch based on val accuracy\n",
      "Starting epoch 10 / 10\n",
      "Step [50/1141]\t Loss: 0.35962936878204343 \t Time: 7.388012170791626 secs [3465.072797417571 ej/sec]]\n",
      "Step [100/1141]\t Loss: 0.3576436173915863 \t Time: 5.30060338973999 secs [4829.638838769213 ej/sec]]\n",
      "Step [150/1141]\t Loss: 0.35831102669239046 \t Time: 5.466755390167236 secs [4682.850827027191 ej/sec]]\n",
      "Step [200/1141]\t Loss: 0.3548092944920063 \t Time: 5.351218223571777 secs [4783.957396324004 ej/sec]]\n",
      "Step [250/1141]\t Loss: 0.35549187994003295 \t Time: 5.297832250595093 secs [4832.165079806823 ej/sec]]\n",
      "Step [300/1141]\t Loss: 0.35524409035841625 \t Time: 5.296258926391602 secs [4833.600538756431 ej/sec]]\n",
      "Step [350/1141]\t Loss: 0.35472008441175734 \t Time: 5.390712022781372 secs [4748.908843917713 ej/sec]]\n",
      "Step [400/1141]\t Loss: 0.35437901206314565 \t Time: 5.356882095336914 secs [4778.899282156017 ej/sec]]\n",
      "Step [450/1141]\t Loss: 0.35386464708381227 \t Time: 5.311071395874023 secs [4820.1197257276 ej/sec]]\n",
      "Step [500/1141]\t Loss: 0.35370548486709597 \t Time: 5.373702049255371 secs [4763.94109039733 ej/sec]]\n",
      "Step [550/1141]\t Loss: 0.3541527070240541 \t Time: 5.349128246307373 secs [4785.826553639328 ej/sec]]\n",
      "Step [600/1141]\t Loss: 0.3547721322874228 \t Time: 5.2505457401275635 secs [4875.683646435206 ej/sec]]\n",
      "Step [650/1141]\t Loss: 0.3541133794417748 \t Time: 5.297991991043091 secs [4832.019384566825 ej/sec]]\n",
      "Step [700/1141]\t Loss: 0.35411334425210955 \t Time: 5.527358531951904 secs [4631.50704844177 ej/sec]]\n",
      "Step [750/1141]\t Loss: 0.35388588734467824 \t Time: 5.262333393096924 secs [4864.762090821122 ej/sec]]\n",
      "Step [800/1141]\t Loss: 0.35436033852398396 \t Time: 5.478536605834961 secs [4672.78067882845 ej/sec]]\n",
      "Step [850/1141]\t Loss: 0.35423043110791375 \t Time: 5.28891921043396 secs [4840.3083846500085 ej/sec]]\n",
      "Step [900/1141]\t Loss: 0.354082534412543 \t Time: 5.420756816864014 secs [4722.587798138853 ej/sec]]\n",
      "Step [950/1141]\t Loss: 0.3536630382035908 \t Time: 5.344454526901245 secs [4790.011753518103 ej/sec]]\n",
      "Step [1000/1141]\t Loss: 0.35368438136577607 \t Time: 5.4006123542785645 secs [4740.203206719463 ej/sec]]\n",
      "Step [1050/1141]\t Loss: 0.35309930860996247 \t Time: 5.338407754898071 secs [4795.4373617323645 ej/sec]]\n",
      "Step [1100/1141]\t Loss: 0.35278801649808883 \t Time: 5.324982643127441 secs [4807.527407256437 ej/sec]]\n",
      "Epoch [10/10]\t Loss: 0.3526107330107041\n",
      "Performing validation...\n",
      "Validation accuracy 0.94196\n",
      "Fine tune done, reloading best model.\n",
      "Computing deep features for test images...\n",
      "Sample IFCB1_2009_001_001602 (1/678) -> Accuracy: 0.9262836807320793\n",
      "Sample IFCB1_2009_001_003939 (2/678) -> Accuracy: 0.9286061194754736\n",
      "Sample IFCB1_2009_006_201718 (3/678) -> Accuracy: 0.8820326678765881\n",
      "Sample IFCB1_2009_006_203845 (4/678) -> Accuracy: 0.9092382495948136\n",
      "Sample IFCB1_2009_006_210219 (5/678) -> Accuracy: 0.9059782608695652\n",
      "Sample IFCB1_2009_022_000342 (6/678) -> Accuracy: 0.8820638820638821\n",
      "Sample IFCB1_2009_022_002654 (7/678) -> Accuracy: 0.8631492168178071\n",
      "Sample IFCB1_2009_022_005008 (8/678) -> Accuracy: 0.8837395125848981\n",
      "Sample IFCB1_2009_034_002158 (9/678) -> Accuracy: 0.9093655589123867\n",
      "Sample IFCB1_2009_034_004443 (10/678) -> Accuracy: 0.8717557251908397\n",
      "Sample IFCB1_2009_041_200046 (11/678) -> Accuracy: 0.9119226638023631\n",
      "Sample IFCB1_2009_041_201714 (12/678) -> Accuracy: 0.9035294117647059\n",
      "Sample IFCB1_2009_041_202359 (13/678) -> Accuracy: 0.9188270293242669\n",
      "Sample IFCB1_2009_041_204230 (14/678) -> Accuracy: 0.9562841530054644\n",
      "Sample IFCB1_2009_041_204709 (15/678) -> Accuracy: 0.9055374592833876\n",
      "Sample IFCB1_2009_041_210044 (16/678) -> Accuracy: 0.8973105134474327\n",
      "Sample IFCB1_2009_051_001203 (17/678) -> Accuracy: 0.9206884913938576\n",
      "Sample IFCB1_2009_051_003517 (18/678) -> Accuracy: 0.9089767733835531\n",
      "Sample IFCB1_2009_051_005834 (19/678) -> Accuracy: 0.912588488765774\n",
      "Sample IFCB1_2009_067_000255 (20/678) -> Accuracy: 0.9621689785624212\n",
      "Sample IFCB1_2009_067_002610 (21/678) -> Accuracy: 0.9541770573566085\n",
      "Sample IFCB1_2009_067_004943 (22/678) -> Accuracy: 0.964031007751938\n",
      "Sample IFCB1_2009_067_180909 (23/678) -> Accuracy: 0.9611470860314524\n",
      "Sample IFCB1_2009_067_183223 (24/678) -> Accuracy: 0.9586854460093897\n",
      "Sample IFCB1_2009_067_185511 (25/678) -> Accuracy: 0.9585591329295505\n",
      "Sample IFCB1_2009_085_171811 (26/678) -> Accuracy: 0.8153018529587567\n",
      "Sample IFCB1_2009_085_174126 (27/678) -> Accuracy: 0.8369683751363141\n",
      "Sample IFCB1_2009_085_180459 (28/678) -> Accuracy: 0.8466135458167331\n",
      "Sample IFCB1_2009_087_001047 (29/678) -> Accuracy: 0.9870550161812298\n",
      "Sample IFCB1_2009_087_001524 (30/678) -> Accuracy: 0.9877380952380952\n",
      "Sample IFCB1_2009_087_003225 (31/678) -> Accuracy: 0.9818181818181818\n",
      "Sample IFCB1_2009_087_003850 (32/678) -> Accuracy: 0.9850838264299803\n",
      "Sample IFCB1_2009_087_005743 (33/678) -> Accuracy: 0.9778325123152709\n",
      "Sample IFCB1_2009_087_182026 (34/678) -> Accuracy: 0.9383717158611742\n",
      "Sample IFCB1_2009_087_184358 (35/678) -> Accuracy: 0.9326963024511841\n",
      "Sample IFCB1_2009_095_000225 (36/678) -> Accuracy: 0.9549202858713579\n",
      "Sample IFCB1_2009_095_000946 (37/678) -> Accuracy: 0.9618845288677831\n",
      "Sample IFCB1_2009_095_002500 (38/678) -> Accuracy: 0.9627408993576018\n",
      "Sample IFCB1_2009_095_003242 (39/678) -> Accuracy: 0.9633606227352033\n",
      "Sample IFCB1_2009_095_004823 (40/678) -> Accuracy: 0.9649568308786186\n",
      "Sample IFCB1_2009_095_005613 (41/678) -> Accuracy: 0.9614534740977166\n",
      "Sample IFCB1_2009_111_001351 (42/678) -> Accuracy: 0.9433170048985304\n",
      "Sample IFCB1_2009_111_002256 (43/678) -> Accuracy: 0.9585247042449547\n",
      "Sample IFCB1_2009_111_003629 (44/678) -> Accuracy: 0.9545313949891742\n",
      "Sample IFCB1_2009_111_004547 (45/678) -> Accuracy: 0.955840114887506\n",
      "Sample IFCB1_2009_111_180423 (46/678) -> Accuracy: 0.9442047742306586\n",
      "Sample IFCB1_2009_111_182348 (47/678) -> Accuracy: 0.9475806451612904\n",
      "Sample IFCB1_2009_111_182751 (48/678) -> Accuracy: 0.9335746891232488\n",
      "Sample IFCB1_2009_111_184401 (49/678) -> Accuracy: 0.947261663286004\n",
      "Sample IFCB1_2009_111_185049 (50/678) -> Accuracy: 0.9413280776228017\n",
      "Sample IFCB1_2009_117_201046 (51/678) -> Accuracy: 0.9496910856134158\n",
      "Sample IFCB1_2009_117_203422 (52/678) -> Accuracy: 0.9567207657095298\n",
      "Sample IFCB1_2009_117_205711 (53/678) -> Accuracy: 0.9557446808510638\n",
      "Sample IFCB1_2009_122_001616 (54/678) -> Accuracy: 0.9431693989071038\n",
      "Sample IFCB1_2009_122_003928 (55/678) -> Accuracy: 0.940254652301665\n",
      "Sample IFCB1_2009_122_160817 (56/678) -> Accuracy: 0.9284595300261097\n",
      "Sample IFCB1_2009_122_163134 (57/678) -> Accuracy: 0.9005847953216374\n",
      "Sample IFCB1_2009_122_165429 (58/678) -> Accuracy: 0.8701923076923077\n",
      "Sample IFCB1_2009_122_221343 (59/678) -> Accuracy: 0.9275634995296331\n",
      "Sample IFCB1_2009_122_223657 (60/678) -> Accuracy: 0.9012797074954296\n",
      "Sample IFCB1_2009_132_182218 (61/678) -> Accuracy: 0.9154589371980676\n",
      "Sample IFCB1_2009_132_184528 (62/678) -> Accuracy: 0.9152542372881356\n",
      "Sample IFCB1_2009_132_190907 (63/678) -> Accuracy: 0.8691049085659288\n",
      "Sample IFCB1_2009_133_001050 (64/678) -> Accuracy: 0.8592057761732852\n",
      "Sample IFCB1_2009_133_003400 (65/678) -> Accuracy: 0.8861016949152543\n",
      "Sample IFCB1_2009_133_005656 (66/678) -> Accuracy: 0.9100765306122449\n",
      "Sample IFCB1_2009_133_181551 (67/678) -> Accuracy: 0.9538461538461539\n",
      "Sample IFCB1_2009_133_183845 (68/678) -> Accuracy: 0.9439579684763573\n",
      "Sample IFCB1_2009_146_002131 (69/678) -> Accuracy: 0.8958262639972854\n",
      "Sample IFCB1_2009_146_004445 (70/678) -> Accuracy: 0.8604289710910786\n",
      "Sample IFCB1_2009_146_180320 (71/678) -> Accuracy: 0.9208255159474672\n",
      "Sample IFCB1_2009_146_182634 (72/678) -> Accuracy: 0.9102304892842701\n",
      "Sample IFCB1_2009_146_184947 (73/678) -> Accuracy: 0.915481832543444\n",
      "Sample IFCB1_2009_155_141455 (74/678) -> Accuracy: 0.9228383094222566\n",
      "Sample IFCB1_2009_155_143810 (75/678) -> Accuracy: 0.9069633608286974\n",
      "Sample IFCB1_2009_155_150121 (76/678) -> Accuracy: 0.8991314646664035\n",
      "Sample IFCB1_2009_158_002209 (77/678) -> Accuracy: 0.8354494667343829\n",
      "Sample IFCB1_2009_158_004522 (78/678) -> Accuracy: 0.80259827906192\n",
      "Sample IFCB1_2009_158_180517 (79/678) -> Accuracy: 0.8944925742574258\n",
      "Sample IFCB1_2009_158_182808 (80/678) -> Accuracy: 0.8852163461538461\n",
      "Sample IFCB1_2009_158_185124 (81/678) -> Accuracy: 0.904375163741158\n",
      "Sample IFCB1_2009_172_000431 (82/678) -> Accuracy: 0.8103150625809236\n",
      "Sample IFCB1_2009_172_002825 (83/678) -> Accuracy: 0.7212328767123287\n",
      "Sample IFCB1_2009_172_005054 (84/678) -> Accuracy: 0.7458563535911602\n",
      "Sample IFCB1_2009_187_233730 (85/678) -> Accuracy: 0.8553199842952494\n",
      "Sample IFCB1_2009_187_235946 (86/678) -> Accuracy: 0.8653739612188366\n",
      "Sample IFCB1_2009_188_002300 (87/678) -> Accuracy: 0.8556168773366566\n",
      "Sample IFCB1_2009_189_001612 (88/678) -> Accuracy: 0.8054569653948536\n",
      "Sample IFCB1_2009_189_004000 (89/678) -> Accuracy: 0.822401394639355\n",
      "Sample IFCB1_2009_189_182114 (90/678) -> Accuracy: 0.8920407601211787\n",
      "Sample IFCB1_2009_189_184429 (91/678) -> Accuracy: 0.8912823112012164\n",
      "Sample IFCB1_2009_195_210249 (92/678) -> Accuracy: 0.9117647058823529\n",
      "Sample IFCB1_2009_195_212603 (93/678) -> Accuracy: 0.9212007504690432\n",
      "Sample IFCB1_2009_195_214914 (94/678) -> Accuracy: 0.9209546377792823\n",
      "Sample IFCB1_2009_206_000452 (95/678) -> Accuracy: 0.8774509803921569\n",
      "Sample IFCB1_2009_206_002109 (96/678) -> Accuracy: 0.8530661809350334\n",
      "Sample IFCB1_2009_206_002806 (97/678) -> Accuracy: 0.8778425655976676\n",
      "Sample IFCB1_2009_206_004500 (98/678) -> Accuracy: 0.871545929798357\n",
      "Sample IFCB1_2009_206_005118 (99/678) -> Accuracy: 0.887785501489573\n",
      "Sample IFCB1_2009_211_153356 (100/678) -> Accuracy: 0.8551257537873217\n",
      "Sample IFCB1_2009_211_155256 (101/678) -> Accuracy: 0.8663484486873508\n",
      "Sample IFCB1_2009_211_155709 (102/678) -> Accuracy: 0.8673218673218673\n",
      "Sample IFCB1_2009_211_161515 (103/678) -> Accuracy: 0.8492268041237113\n",
      "Sample IFCB1_2009_211_162022 (104/678) -> Accuracy: 0.87729117112075\n",
      "Sample IFCB1_2009_211_163909 (105/678) -> Accuracy: 0.8919449901768173\n",
      "Sample IFCB1_2009_215_000240 (106/678) -> Accuracy: 0.903329602686066\n",
      "Sample IFCB1_2009_215_002526 (107/678) -> Accuracy: 0.8992313067784766\n",
      "Sample IFCB1_2009_215_004836 (108/678) -> Accuracy: 0.9003302311618133\n",
      "Sample IFCB1_2009_224_161052 (109/678) -> Accuracy: 0.8387677535507101\n",
      "Sample IFCB1_2009_224_163310 (110/678) -> Accuracy: 0.8836263257300596\n",
      "Sample IFCB1_2009_224_165556 (111/678) -> Accuracy: 0.8830932736680185\n",
      "Sample IFCB1_2009_228_000456 (112/678) -> Accuracy: 0.8132596685082873\n",
      "Sample IFCB1_2009_228_002831 (113/678) -> Accuracy: 0.8065395095367848\n",
      "Sample IFCB1_2009_228_005118 (114/678) -> Accuracy: 0.864912668582799\n",
      "Sample IFCB1_2009_230_120831 (115/678) -> Accuracy: 0.7883522727272727\n",
      "Sample IFCB1_2009_230_123119 (116/678) -> Accuracy: 0.8030544791429223\n",
      "Sample IFCB1_2009_230_125432 (117/678) -> Accuracy: 0.8283491368830538\n",
      "Sample IFCB1_2009_247_153949 (118/678) -> Accuracy: 0.9197163120567375\n",
      "Sample IFCB1_2009_247_160304 (119/678) -> Accuracy: 0.9409120951751487\n",
      "Sample IFCB1_2009_247_162615 (120/678) -> Accuracy: 0.9190023752969121\n",
      "Sample IFCB1_2009_249_000342 (121/678) -> Accuracy: 0.9184952978056427\n",
      "Sample IFCB1_2009_249_002654 (122/678) -> Accuracy: 0.916935283907761\n",
      "Sample IFCB1_2009_249_005006 (123/678) -> Accuracy: 0.9227383863080685\n",
      "Sample IFCB1_2009_249_180840 (124/678) -> Accuracy: 0.9480209281164695\n",
      "Sample IFCB1_2009_249_183154 (125/678) -> Accuracy: 0.9489669188212713\n",
      "Sample IFCB1_2009_249_185505 (126/678) -> Accuracy: 0.9505951044239839\n",
      "Sample IFCB1_2009_262_001431 (127/678) -> Accuracy: 0.9289727704941652\n",
      "Sample IFCB1_2009_262_003744 (128/678) -> Accuracy: 0.9226270805218174\n",
      "Sample IFCB1_2009_262_181939 (129/678) -> Accuracy: 0.8606899582509339\n",
      "Sample IFCB1_2009_262_184307 (130/678) -> Accuracy: 0.8351948940050148\n",
      "Sample IFCB1_2009_267_002056 (131/678) -> Accuracy: 0.9224035017906884\n",
      "Sample IFCB1_2009_267_004410 (132/678) -> Accuracy: 0.91532582461786\n",
      "Sample IFCB1_2009_275_125437 (133/678) -> Accuracy: 0.9394762366634336\n",
      "Sample IFCB1_2009_275_131750 (134/678) -> Accuracy: 0.93359375\n",
      "Sample IFCB1_2009_275_134122 (135/678) -> Accuracy: 0.9358068315665489\n",
      "Sample IFCB1_2009_276_000655 (136/678) -> Accuracy: 0.8987320928700807\n",
      "Sample IFCB1_2009_276_003007 (137/678) -> Accuracy: 0.9079311404857056\n",
      "Sample IFCB1_2009_276_005321 (138/678) -> Accuracy: 0.9184466019417475\n",
      "Sample IFCB1_2009_276_181153 (139/678) -> Accuracy: 0.9456326204277433\n",
      "Sample IFCB1_2009_276_183506 (140/678) -> Accuracy: 0.9451926674148896\n",
      "Sample IFCB1_2009_276_185819 (141/678) -> Accuracy: 0.9490038384207641\n",
      "Sample IFCB1_2009_288_130947 (142/678) -> Accuracy: 0.9505265249354262\n",
      "Sample IFCB1_2009_288_133316 (143/678) -> Accuracy: 0.9316851008458035\n",
      "Sample IFCB1_2009_288_193443 (144/678) -> Accuracy: 0.9326571333555038\n",
      "Sample IFCB1_2009_288_195758 (145/678) -> Accuracy: 0.8904461113304382\n",
      "Sample IFCB1_2009_288_202112 (146/678) -> Accuracy: 0.8714436248682824\n",
      "Sample IFCB1_2009_292_001605 (147/678) -> Accuracy: 0.9191596254852706\n",
      "Sample IFCB1_2009_292_003918 (148/678) -> Accuracy: 0.9172814665772412\n",
      "Sample IFCB1_2009_292_182104 (149/678) -> Accuracy: 0.9142931800365822\n",
      "Sample IFCB1_2009_292_184417 (150/678) -> Accuracy: 0.9230769230769231\n",
      "Sample IFCB1_2009_294_145539 (151/678) -> Accuracy: 0.905722429291822\n",
      "Sample IFCB1_2009_294_151853 (152/678) -> Accuracy: 0.9107698692674829\n",
      "Sample IFCB1_2009_294_154206 (153/678) -> Accuracy: 0.9156933531535357\n",
      "Sample IFCB1_2009_307_000512 (154/678) -> Accuracy: 0.9239312657166806\n",
      "Sample IFCB1_2009_307_002826 (155/678) -> Accuracy: 0.929553264604811\n",
      "Sample IFCB1_2009_307_005137 (156/678) -> Accuracy: 0.9246485061511424\n",
      "Sample IFCB1_2009_307_181012 (157/678) -> Accuracy: 0.9607843137254902\n",
      "Sample IFCB1_2009_307_183326 (158/678) -> Accuracy: 0.9569761365615747\n",
      "Sample IFCB1_2009_307_185636 (159/678) -> Accuracy: 0.9552880481513327\n",
      "Sample IFCB1_2009_309_180858 (160/678) -> Accuracy: 0.958069508934853\n",
      "Sample IFCB1_2009_309_183212 (161/678) -> Accuracy: 0.9628877150209941\n",
      "Sample IFCB1_2009_309_185523 (162/678) -> Accuracy: 0.9661016949152542\n",
      "Sample IFCB1_2009_328_001438 (163/678) -> Accuracy: 0.9670237929595102\n",
      "Sample IFCB1_2009_328_003811 (164/678) -> Accuracy: 0.9649963689179375\n",
      "Sample IFCB1_2009_328_181957 (165/678) -> Accuracy: 0.9667662434652726\n",
      "Sample IFCB1_2009_328_184248 (166/678) -> Accuracy: 0.9675696796040636\n",
      "Sample IFCB1_2009_341_000206 (167/678) -> Accuracy: 0.941145467322558\n",
      "Sample IFCB1_2009_341_002518 (168/678) -> Accuracy: 0.9400162999185004\n",
      "Sample IFCB1_2009_341_004829 (169/678) -> Accuracy: 0.9353152564521399\n",
      "Sample IFCB1_2009_362_002638 (170/678) -> Accuracy: 0.8718528082633957\n",
      "Sample IFCB1_2009_362_004552 (171/678) -> Accuracy: 0.8759625962596259\n",
      "Sample IFCB1_2010_012_190006 (172/678) -> Accuracy: 0.8050209205020921\n",
      "Sample IFCB1_2010_012_191736 (173/678) -> Accuracy: 0.8071895424836601\n",
      "Sample IFCB1_2010_012_192259 (174/678) -> Accuracy: 0.8046666666666666\n",
      "Sample IFCB1_2010_012_194611 (175/678) -> Accuracy: 0.7243207422133864\n",
      "Sample IFCB1_2010_028_061806 (176/678) -> Accuracy: 0.8043478260869565\n",
      "Sample IFCB1_2010_028_062254 (177/678) -> Accuracy: 0.8183161004431314\n",
      "Sample IFCB1_2010_028_064151 (178/678) -> Accuracy: 0.7840531561461794\n",
      "Sample IFCB1_2010_028_064623 (179/678) -> Accuracy: 0.8186094069529652\n",
      "Sample IFCB1_2010_034_140026 (180/678) -> Accuracy: 0.7454050374404356\n",
      "Sample IFCB1_2010_034_141639 (181/678) -> Accuracy: 0.7405900305188199\n",
      "Sample IFCB1_2010_034_142321 (182/678) -> Accuracy: 0.7203334064487826\n",
      "Sample IFCB1_2010_034_144006 (183/678) -> Accuracy: 0.7486687965921193\n",
      "Sample IFCB1_2010_034_144636 (184/678) -> Accuracy: 0.7315392895586652\n",
      "Sample IFCB1_2010_036_175129 (185/678) -> Accuracy: 0.7530261201953705\n",
      "Sample IFCB1_2010_036_181100 (186/678) -> Accuracy: 0.7887323943661971\n",
      "Sample IFCB1_2010_036_181442 (187/678) -> Accuracy: 0.7614202657807309\n",
      "Sample IFCB1_2010_036_183757 (188/678) -> Accuracy: 0.7320073815870413\n",
      "Sample IFCB1_2010_036_185706 (189/678) -> Accuracy: 0.7446043165467626\n",
      "Sample IFCB1_2010_040_060201 (190/678) -> Accuracy: 0.6717776712985146\n",
      "Sample IFCB1_2010_040_210751 (191/678) -> Accuracy: 0.788546255506608\n",
      "Sample IFCB1_2010_040_212550 (192/678) -> Accuracy: 0.8073260073260073\n",
      "Sample IFCB1_2010_040_214242 (193/678) -> Accuracy: 0.8611632270168855\n",
      "Sample IFCB1_2010_040_214828 (194/678) -> Accuracy: 0.7409037558685446\n",
      "Sample IFCB1_2010_040_220310 (195/678) -> Accuracy: 0.7415891195418755\n",
      "Sample IFCB1_2010_044_170700 (196/678) -> Accuracy: 0.8539778449144008\n",
      "Sample IFCB1_2010_044_172801 (197/678) -> Accuracy: 0.8535602328705777\n",
      "Sample IFCB1_2010_044_175113 (198/678) -> Accuracy: 0.8242349048800661\n",
      "Sample IFCB1_2010_045_182248 (199/678) -> Accuracy: 0.9038461538461539\n",
      "Sample IFCB1_2010_045_184626 (200/678) -> Accuracy: 0.8998596162845109\n",
      "Sample IFCB1_2010_055_011758 (201/678) -> Accuracy: 0.9727655360237683\n",
      "Sample IFCB1_2010_055_014047 (202/678) -> Accuracy: 0.9711055276381909\n",
      "Sample IFCB1_2010_060_221135 (203/678) -> Accuracy: 0.9445005045408678\n",
      "Sample IFCB1_2010_060_223450 (204/678) -> Accuracy: 0.9485414618157981\n",
      "Sample IFCB1_2010_060_225802 (205/678) -> Accuracy: 0.9521849271690943\n",
      "Sample IFCB1_2010_078_181113 (206/678) -> Accuracy: 0.9389515593895156\n",
      "Sample IFCB1_2010_078_183357 (207/678) -> Accuracy: 0.9315642458100558\n",
      "Sample IFCB1_2010_078_185738 (208/678) -> Accuracy: 0.9276750330250991\n",
      "Sample IFCB1_2010_096_081043 (209/678) -> Accuracy: 0.9104549854791868\n",
      "Sample IFCB1_2010_096_083354 (210/678) -> Accuracy: 0.8952520802741067\n",
      "Sample IFCB1_2010_096_085706 (211/678) -> Accuracy: 0.9012707722385142\n",
      "Sample IFCB1_2010_107_120314 (212/678) -> Accuracy: 0.8862828713574982\n",
      "Sample IFCB1_2010_107_122604 (213/678) -> Accuracy: 0.890625\n",
      "Sample IFCB1_2010_107_124914 (214/678) -> Accuracy: 0.8793526238352133\n",
      "Sample IFCB1_2010_132_164328 (215/678) -> Accuracy: 0.9044333529066354\n",
      "Sample IFCB1_2010_132_170642 (216/678) -> Accuracy: 0.9089086221712976\n",
      "Sample IFCB1_2010_132_185641 (217/678) -> Accuracy: 0.8864179104477612\n",
      "Sample IFCB1_2010_141_161721 (218/678) -> Accuracy: 0.9082289803220036\n",
      "Sample IFCB1_2010_141_164059 (219/678) -> Accuracy: 0.8996131528046422\n",
      "Sample IFCB1_2010_141_170349 (220/678) -> Accuracy: 0.909637347225005\n",
      "Sample IFCB1_2010_160_130123 (221/678) -> Accuracy: 0.9525566684238271\n",
      "Sample IFCB1_2010_160_132435 (222/678) -> Accuracy: 0.942322286870731\n",
      "Sample IFCB1_2010_160_134749 (223/678) -> Accuracy: 0.9437229437229437\n",
      "Sample IFCB1_2010_174_080447 (224/678) -> Accuracy: 0.8776002971768202\n",
      "Sample IFCB1_2010_174_082758 (225/678) -> Accuracy: 0.8700851236319417\n",
      "Sample IFCB1_2010_174_085113 (226/678) -> Accuracy: 0.8859613009468917\n",
      "Sample IFCB1_2010_189_001111 (227/678) -> Accuracy: 0.8630393996247655\n",
      "Sample IFCB1_2010_189_003444 (228/678) -> Accuracy: 0.8761061946902655\n",
      "Sample IFCB1_2010_189_005739 (229/678) -> Accuracy: 0.8405511811023622\n",
      "Sample IFCB1_2010_189_012033 (230/678) -> Accuracy: 0.8610687022900764\n",
      "Sample IFCB1_2010_204_174940 (231/678) -> Accuracy: 0.8648648648648649\n",
      "Sample IFCB1_2010_204_181254 (232/678) -> Accuracy: 0.8854435831180018\n",
      "Sample IFCB1_2010_204_183605 (233/678) -> Accuracy: 0.8517699115044248\n",
      "Sample IFCB1_2010_208_222407 (234/678) -> Accuracy: 0.8921200750469043\n",
      "Sample IFCB1_2010_208_224741 (235/678) -> Accuracy: 0.9212007504690432\n",
      "Sample IFCB1_2010_208_231035 (236/678) -> Accuracy: 0.9201596806387226\n",
      "Sample IFCB1_2010_208_233403 (237/678) -> Accuracy: 0.9261477045908184\n",
      "Sample IFCB1_2010_225_104216 (238/678) -> Accuracy: 0.8465286236297198\n",
      "Sample IFCB1_2010_225_105359 (239/678) -> Accuracy: 0.8724226804123711\n",
      "Sample IFCB1_2010_225_111714 (240/678) -> Accuracy: 0.87900466562986\n",
      "Sample IFCB1_2010_285_001802 (241/678) -> Accuracy: 0.9116465863453815\n",
      "Sample IFCB1_2010_285_002233 (242/678) -> Accuracy: 0.924896265560166\n",
      "Sample IFCB1_2010_285_004141 (243/678) -> Accuracy: 0.8891891891891892\n",
      "Sample IFCB1_2010_285_004546 (244/678) -> Accuracy: 0.9189737643136686\n",
      "Sample IFCB1_2010_285_010422 (245/678) -> Accuracy: 0.9101899827288429\n",
      "Sample IFCB1_2010_285_010858 (246/678) -> Accuracy: 0.924903368304804\n",
      "Sample IFCB1_2010_285_012822 (247/678) -> Accuracy: 0.8871595330739299\n",
      "Sample IFCB1_2010_285_013211 (248/678) -> Accuracy: 0.9310038495706248\n",
      "Sample IFCB1_2010_295_054038 (249/678) -> Accuracy: 0.8198606271777004\n",
      "Sample IFCB1_2010_295_060351 (250/678) -> Accuracy: 0.8713550600343053\n",
      "Sample IFCB1_2010_295_062703 (251/678) -> Accuracy: 0.8793449197860963\n",
      "Sample IFCB1_2010_306_192632 (252/678) -> Accuracy: 0.8659517426273459\n",
      "Sample IFCB1_2010_306_194946 (253/678) -> Accuracy: 0.8620608899297424\n",
      "Sample IFCB1_2010_306_201258 (254/678) -> Accuracy: 0.8541121005762179\n",
      "Sample IFCB1_2011_218_192314 (255/678) -> Accuracy: 0.9137020077492075\n",
      "Sample IFCB1_2011_218_194628 (256/678) -> Accuracy: 0.9043768676393039\n",
      "Sample IFCB1_2011_218_200940 (257/678) -> Accuracy: 0.9111328125\n",
      "Sample IFCB1_2011_232_164318 (258/678) -> Accuracy: 0.9359065155807366\n",
      "Sample IFCB1_2011_232_170633 (259/678) -> Accuracy: 0.9347507331378299\n",
      "Sample IFCB1_2011_232_172943 (260/678) -> Accuracy: 0.9220618556701031\n",
      "Sample IFCB1_2011_255_201526 (261/678) -> Accuracy: 0.9309319233311302\n",
      "Sample IFCB1_2011_255_203841 (262/678) -> Accuracy: 0.9276672694394213\n",
      "Sample IFCB1_2011_255_210152 (263/678) -> Accuracy: 0.9242902208201893\n",
      "Sample IFCB1_2011_255_212504 (264/678) -> Accuracy: 0.914733016494269\n",
      "Sample IFCB1_2011_260_221456 (265/678) -> Accuracy: 0.8947658402203856\n",
      "Sample IFCB1_2011_260_223806 (266/678) -> Accuracy: 0.9095460614152203\n",
      "Sample IFCB1_2011_260_230119 (267/678) -> Accuracy: 0.8925799441167339\n",
      "Sample IFCB1_2011_264_205348 (268/678) -> Accuracy: 0.9262072707542051\n",
      "Sample IFCB1_2011_264_211701 (269/678) -> Accuracy: 0.9071005917159763\n",
      "Sample IFCB1_2011_264_214016 (270/678) -> Accuracy: 0.8873325213154689\n",
      "Sample IFCB1_2011_270_185023 (271/678) -> Accuracy: 0.9312560231288146\n",
      "Sample IFCB1_2011_270_191038 (272/678) -> Accuracy: 0.92040871201936\n",
      "Sample IFCB1_2011_270_193351 (273/678) -> Accuracy: 0.9182971972501323\n",
      "Sample IFCB1_2011_279_215203 (274/678) -> Accuracy: 0.839787632729544\n",
      "Sample IFCB1_2011_279_221512 (275/678) -> Accuracy: 0.8420733555622916\n",
      "Sample IFCB1_2011_279_223825 (276/678) -> Accuracy: 0.808985234055922\n",
      "Sample IFCB1_2011_284_210748 (277/678) -> Accuracy: 0.746763498579097\n",
      "Sample IFCB1_2011_284_212925 (278/678) -> Accuracy: 0.7856832027850305\n",
      "Sample IFCB1_2011_284_215233 (279/678) -> Accuracy: 0.7993168232280102\n",
      "Sample IFCB1_2012_111_200449 (280/678) -> Accuracy: 0.8346206269877329\n",
      "Sample IFCB1_2012_111_202805 (281/678) -> Accuracy: 0.8406593406593407\n",
      "Sample IFCB1_2012_111_205116 (282/678) -> Accuracy: 0.8323723228995058\n",
      "Sample IFCB1_2012_122_205827 (283/678) -> Accuracy: 0.8749484110606686\n",
      "Sample IFCB1_2012_122_211654 (284/678) -> Accuracy: 0.8948655256723717\n",
      "Sample IFCB1_2012_122_212138 (285/678) -> Accuracy: 0.888354186717998\n",
      "Sample IFCB1_2012_122_214453 (286/678) -> Accuracy: 0.881335952848723\n",
      "Sample IFCB1_2012_142_015832 (287/678) -> Accuracy: 0.9321450906816761\n",
      "Sample IFCB1_2012_142_022144 (288/678) -> Accuracy: 0.9379136463914277\n",
      "Sample IFCB1_2012_142_024457 (289/678) -> Accuracy: 0.9382405745062836\n",
      "Sample IFCB1_2012_151_152004 (290/678) -> Accuracy: 0.7824104234527687\n",
      "Sample IFCB1_2012_151_154317 (291/678) -> Accuracy: 0.7745762711864407\n",
      "Sample IFCB1_2012_151_160629 (292/678) -> Accuracy: 0.7912995594713657\n",
      "Sample IFCB1_2012_167_161748 (293/678) -> Accuracy: 0.8803334968121628\n",
      "Sample IFCB1_2012_167_164059 (294/678) -> Accuracy: 0.8834765998089781\n",
      "Sample IFCB1_2012_167_170412 (295/678) -> Accuracy: 0.8936170212765957\n",
      "Sample IFCB1_2012_177_204001 (296/678) -> Accuracy: 0.9195759458965455\n",
      "Sample IFCB1_2012_177_210316 (297/678) -> Accuracy: 0.909622641509434\n",
      "Sample IFCB1_2012_177_212627 (298/678) -> Accuracy: 0.9157303370786517\n",
      "Sample IFCB1_2012_258_172757 (299/678) -> Accuracy: 0.8696098562628337\n",
      "Sample IFCB1_2012_258_175109 (300/678) -> Accuracy: 0.8529411764705882\n",
      "Sample IFCB1_2012_258_181419 (301/678) -> Accuracy: 0.8672145328719724\n",
      "Sample IFCB1_2012_269_201802 (302/678) -> Accuracy: 0.9092195713708047\n",
      "Sample IFCB1_2012_269_204116 (303/678) -> Accuracy: 0.9135211766790714\n",
      "Sample IFCB1_2012_269_210427 (304/678) -> Accuracy: 0.917440486240081\n",
      "Sample IFCB1_2012_282_200950 (305/678) -> Accuracy: 0.7389100126742713\n",
      "Sample IFCB1_2012_282_203258 (306/678) -> Accuracy: 0.7475845410628019\n",
      "Sample IFCB1_2012_282_205545 (307/678) -> Accuracy: 0.7711962833914053\n",
      "Sample IFCB1_2012_292_090941 (308/678) -> Accuracy: 0.7681718061674009\n",
      "Sample IFCB1_2012_292_093250 (309/678) -> Accuracy: 0.7749586321014893\n",
      "Sample IFCB1_2012_292_095605 (310/678) -> Accuracy: 0.7795669072737368\n",
      "Sample IFCB1_2012_311_155904 (311/678) -> Accuracy: 0.8385590393595731\n",
      "Sample IFCB1_2012_311_162214 (312/678) -> Accuracy: 0.8363514419852448\n",
      "Sample IFCB1_2012_311_164530 (313/678) -> Accuracy: 0.8155149934810951\n",
      "Sample IFCB1_2012_331_150331 (314/678) -> Accuracy: 0.8746192893401015\n",
      "Sample IFCB1_2012_331_152623 (315/678) -> Accuracy: 0.8886792452830189\n",
      "Sample IFCB1_2012_331_154955 (316/678) -> Accuracy: 0.8781028368794326\n",
      "Sample IFCB1_2012_350_065434 (317/678) -> Accuracy: 0.8955847255369929\n",
      "Sample IFCB1_2012_350_071744 (318/678) -> Accuracy: 0.8937426210153483\n",
      "Sample IFCB1_2012_350_074040 (319/678) -> Accuracy: 0.902206320810972\n",
      "Sample IFCB1_2012_354_200806 (320/678) -> Accuracy: 0.8913601368691189\n",
      "Sample IFCB1_2012_354_203118 (321/678) -> Accuracy: 0.8988941548183255\n",
      "Sample IFCB1_2012_354_205434 (322/678) -> Accuracy: 0.9105485232067511\n",
      "Sample IFCB1_2013_001_075331 (323/678) -> Accuracy: 0.8991262907069102\n",
      "Sample IFCB1_2013_001_081645 (324/678) -> Accuracy: 0.8786885245901639\n",
      "Sample IFCB1_2013_001_083957 (325/678) -> Accuracy: 0.888\n",
      "Sample IFCB1_2013_011_174553 (326/678) -> Accuracy: 0.8589830508474576\n",
      "Sample IFCB1_2013_011_180852 (327/678) -> Accuracy: 0.8678756476683938\n",
      "Sample IFCB1_2013_011_183214 (328/678) -> Accuracy: 0.8561601000625391\n",
      "Sample IFCB1_2013_025_191226 (329/678) -> Accuracy: 0.8786565547128927\n",
      "Sample IFCB1_2013_025_193537 (330/678) -> Accuracy: 0.857451403887689\n",
      "Sample IFCB1_2013_025_195855 (331/678) -> Accuracy: 0.8635838150289017\n",
      "Sample IFCB1_2013_045_175844 (332/678) -> Accuracy: 0.8231046931407943\n",
      "Sample IFCB1_2013_045_182213 (333/678) -> Accuracy: 0.8220502901353965\n",
      "Sample IFCB1_2013_045_184523 (334/678) -> Accuracy: 0.841897233201581\n",
      "Sample IFCB1_2013_057_131300 (335/678) -> Accuracy: 0.8047722342733189\n",
      "Sample IFCB1_2013_057_133610 (336/678) -> Accuracy: 0.7653276955602537\n",
      "Sample IFCB1_2013_057_135924 (337/678) -> Accuracy: 0.7662337662337663\n",
      "Sample IFCB1_2013_288_171044 (338/678) -> Accuracy: 0.9178365937859608\n",
      "Sample IFCB1_2013_288_173355 (339/678) -> Accuracy: 0.9216003700277521\n",
      "Sample IFCB1_2013_288_175708 (340/678) -> Accuracy: 0.9126526849504494\n",
      "Sample IFCB1_2013_302_161504 (341/678) -> Accuracy: 0.9110919292188174\n",
      "Sample IFCB1_2013_302_163814 (342/678) -> Accuracy: 0.9164118246687054\n",
      "Sample IFCB1_2013_302_170210 (343/678) -> Accuracy: 0.9123400500073541\n",
      "Sample IFCB1_2013_302_172158 (344/678) -> Accuracy: 0.9196428571428571\n",
      "Sample IFCB1_2013_314_061144 (345/678) -> Accuracy: 0.862231436110064\n",
      "Sample IFCB1_2013_314_063456 (346/678) -> Accuracy: 0.8746895893027699\n",
      "Sample IFCB1_2013_314_065810 (347/678) -> Accuracy: 0.8653662268120099\n",
      "Sample IFCB1_2014_150_192358 (348/678) -> Accuracy: 0.912826420890937\n",
      "Sample IFCB1_2014_150_194713 (349/678) -> Accuracy: 0.9086172344689378\n",
      "Sample IFCB1_2014_150_203337 (350/678) -> Accuracy: 0.9065455921301552\n",
      "Sample IFCB1_2014_156_180619 (351/678) -> Accuracy: 0.9015892420537898\n",
      "Sample IFCB1_2014_156_182931 (352/678) -> Accuracy: 0.9069031032298923\n",
      "Sample IFCB1_2014_156_185244 (353/678) -> Accuracy: 0.9083308450283667\n",
      "Sample IFCB1_2014_176_150724 (354/678) -> Accuracy: 0.9263738595374497\n",
      "Sample IFCB1_2014_176_153037 (355/678) -> Accuracy: 0.9289924973204716\n",
      "Sample IFCB1_2014_176_155350 (356/678) -> Accuracy: 0.9201183431952663\n",
      "Sample IFCB1_2014_188_215702 (357/678) -> Accuracy: 0.8918529199711608\n",
      "Sample IFCB1_2014_188_221614 (358/678) -> Accuracy: 0.9159420289855073\n",
      "Sample IFCB1_2014_188_222013 (359/678) -> Accuracy: 0.9002479218317049\n",
      "Sample IFCB1_2014_188_224330 (360/678) -> Accuracy: 0.8935887988209286\n",
      "Sample IFCB1_2014_270_210526 (361/678) -> Accuracy: 0.8877349159248269\n",
      "Sample IFCB1_2014_270_212839 (362/678) -> Accuracy: 0.9068100358422939\n",
      "Sample IFCB1_2014_270_215153 (363/678) -> Accuracy: 0.8964948453608247\n",
      "Sample IFCB1_2014_277_165430 (364/678) -> Accuracy: 0.9104055328513047\n",
      "Sample IFCB1_2014_277_171744 (365/678) -> Accuracy: 0.8968174204355109\n",
      "Sample IFCB1_2014_277_174057 (366/678) -> Accuracy: 0.9047272727272727\n",
      "Sample IFCB1_2014_304_175803 (367/678) -> Accuracy: 0.9293924466338259\n",
      "Sample IFCB1_2014_304_182116 (368/678) -> Accuracy: 0.9267552182163188\n",
      "Sample IFCB1_2014_304_184428 (369/678) -> Accuracy: 0.9177153920619555\n",
      "Sample IFCB5_2010_239_185422 (370/678) -> Accuracy: 0.7608738437315489\n",
      "Sample IFCB5_2010_239_191241 (371/678) -> Accuracy: 0.754874651810585\n",
      "Sample IFCB5_2010_239_191708 (372/678) -> Accuracy: 0.7886004294358774\n",
      "Sample IFCB5_2010_239_193436 (373/678) -> Accuracy: 0.7777777777777778\n",
      "Sample IFCB5_2010_239_194047 (374/678) -> Accuracy: 0.7879219194794632\n",
      "Sample IFCB5_2010_259_182514 (375/678) -> Accuracy: 0.7947819314641744\n",
      "Sample IFCB5_2010_259_184200 (376/678) -> Accuracy: 0.8071176310785113\n",
      "Sample IFCB5_2010_259_190513 (377/678) -> Accuracy: 0.7996554270243662\n",
      "Sample IFCB5_2010_271_034223 (378/678) -> Accuracy: 0.5305420131918555\n",
      "Sample IFCB5_2010_271_040557 (379/678) -> Accuracy: 0.5325623858794888\n",
      "Sample IFCB5_2010_271_042911 (380/678) -> Accuracy: 0.5270350564468211\n",
      "Sample IFCB5_2010_320_201135 (381/678) -> Accuracy: 0.9056156968876861\n",
      "Sample IFCB5_2010_320_211911 (382/678) -> Accuracy: 0.8696721311475409\n",
      "Sample IFCB5_2010_320_212330 (383/678) -> Accuracy: 0.8706254233461278\n",
      "Sample IFCB5_2010_320_213957 (384/678) -> Accuracy: 0.7721556443260837\n",
      "Sample IFCB5_2010_345_120048 (385/678) -> Accuracy: 0.7698821796759941\n",
      "Sample IFCB5_2010_345_122401 (386/678) -> Accuracy: 0.7906634809777614\n",
      "Sample IFCB5_2010_345_124713 (387/678) -> Accuracy: 0.8101972459992557\n",
      "Sample IFCB5_2010_353_031457 (388/678) -> Accuracy: 0.8164383561643835\n",
      "Sample IFCB5_2010_353_031931 (389/678) -> Accuracy: 0.8598212339219533\n",
      "Sample IFCB5_2010_353_033706 (390/678) -> Accuracy: 0.8650793650793651\n",
      "Sample IFCB5_2010_353_034243 (391/678) -> Accuracy: 0.864955826672276\n",
      "Sample IFCB5_2010_353_040109 (392/678) -> Accuracy: 0.874384236453202\n",
      "Sample IFCB5_2011_006_180709 (393/678) -> Accuracy: 0.8899835796387521\n",
      "Sample IFCB5_2011_006_181617 (394/678) -> Accuracy: 0.8750308109440473\n",
      "Sample IFCB5_2011_006_183931 (395/678) -> Accuracy: 0.8558929490478642\n",
      "Sample IFCB5_2011_024_091607 (396/678) -> Accuracy: 0.7094298245614035\n",
      "Sample IFCB5_2011_024_092305 (397/678) -> Accuracy: 0.75859291084855\n",
      "Sample IFCB5_2011_024_093952 (398/678) -> Accuracy: 0.7225274725274725\n",
      "Sample IFCB5_2011_024_094617 (399/678) -> Accuracy: 0.7060726729716277\n",
      "Sample IFCB5_2011_039_030924 (400/678) -> Accuracy: 0.7789804469273743\n",
      "Sample IFCB5_2011_039_031934 (401/678) -> Accuracy: 0.7563537783802101\n",
      "Sample IFCB5_2011_039_033238 (402/678) -> Accuracy: 0.757901596611274\n",
      "Sample IFCB5_2011_039_034322 (403/678) -> Accuracy: 0.5760830648048694\n",
      "Sample IFCB5_2011_039_035551 (404/678) -> Accuracy: 0.7544204322200393\n",
      "Sample IFCB5_2011_039_040624 (405/678) -> Accuracy: 0.745190685116436\n",
      "Sample IFCB5_2011_051_190633 (406/678) -> Accuracy: 0.7784660766961652\n",
      "Sample IFCB5_2011_051_192947 (407/678) -> Accuracy: 0.7506670619626445\n",
      "Sample IFCB5_2011_051_195300 (408/678) -> Accuracy: 0.7472913616398243\n",
      "Sample IFCB5_2011_063_212045 (409/678) -> Accuracy: 0.7816813048933501\n",
      "Sample IFCB5_2011_063_214358 (410/678) -> Accuracy: 0.7971473851030111\n",
      "Sample IFCB5_2011_080_001733 (411/678) -> Accuracy: 0.8532763532763533\n",
      "Sample IFCB5_2011_082_141705 (412/678) -> Accuracy: 0.8494558645707376\n",
      "Sample IFCB5_2011_082_160706 (413/678) -> Accuracy: 0.8916272788656313\n",
      "Sample IFCB5_2011_082_163019 (414/678) -> Accuracy: 0.9077784325279906\n",
      "Sample IFCB5_2011_082_182624 (415/678) -> Accuracy: 0.9068373740125306\n",
      "Sample IFCB5_2011_082_184936 (416/678) -> Accuracy: 0.9109421667119196\n",
      "Sample IFCB5_2011_096_091011 (417/678) -> Accuracy: 0.9033228301346319\n",
      "Sample IFCB5_2011_096_092939 (418/678) -> Accuracy: 0.8762376237623762\n",
      "Sample IFCB5_2011_096_093323 (419/678) -> Accuracy: 0.8981944444444444\n",
      "Sample IFCB5_2011_096_095637 (420/678) -> Accuracy: 0.9007887090078871\n",
      "Sample IFCB5_2011_113_061524 (421/678) -> Accuracy: 0.8951710261569417\n",
      "Sample IFCB5_2011_113_063838 (422/678) -> Accuracy: 0.8477424579874469\n",
      "Sample IFCB5_2011_123_190656 (423/678) -> Accuracy: 0.7968069666182874\n",
      "Sample IFCB5_2011_123_193008 (424/678) -> Accuracy: 0.7992565055762082\n",
      "Sample IFCB5_2011_141_101135 (425/678) -> Accuracy: 0.8607342378292099\n",
      "Sample IFCB5_2011_141_103449 (426/678) -> Accuracy: 0.8584250144759699\n",
      "Sample IFCB5_2011_141_105801 (427/678) -> Accuracy: 0.8402095612311722\n",
      "Sample IFCB5_2011_157_221638 (428/678) -> Accuracy: 0.8843956043956044\n",
      "Sample IFCB5_2011_157_223951 (429/678) -> Accuracy: 0.888507718696398\n",
      "Sample IFCB5_2011_157_230304 (430/678) -> Accuracy: 0.8884924174843889\n",
      "Sample IFCB5_2011_167_020552 (431/678) -> Accuracy: 0.8334921391138638\n",
      "Sample IFCB5_2011_167_022908 (432/678) -> Accuracy: 0.8294075660242684\n",
      "Sample IFCB5_2011_167_025219 (433/678) -> Accuracy: 0.8161801088570014\n",
      "Sample IFCB5_2011_187_155432 (434/678) -> Accuracy: 0.6957906712172923\n",
      "Sample IFCB5_2011_187_161745 (435/678) -> Accuracy: 0.7113223854796888\n",
      "Sample IFCB5_2011_187_164058 (436/678) -> Accuracy: 0.6937819420783645\n",
      "Sample IFCB5_2011_187_170411 (437/678) -> Accuracy: 0.763241863433312\n",
      "Sample IFCB5_2011_187_172725 (438/678) -> Accuracy: 0.7682119205298014\n",
      "Sample IFCB5_2011_187_175039 (439/678) -> Accuracy: 0.7294851166532582\n",
      "Sample IFCB5_2011_187_181351 (440/678) -> Accuracy: 0.750796812749004\n",
      "Sample IFCB5_2011_187_183704 (441/678) -> Accuracy: 0.727900833163991\n",
      "Sample IFCB5_2011_187_190021 (442/678) -> Accuracy: 0.742755765819042\n",
      "Sample IFCB5_2011_187_192331 (443/678) -> Accuracy: 0.7248891459417776\n",
      "Sample IFCB5_2011_188_150310 (444/678) -> Accuracy: 0.6901050491358861\n",
      "Sample IFCB5_2011_188_152623 (445/678) -> Accuracy: 0.6882634094530006\n",
      "Sample IFCB5_2011_188_154936 (446/678) -> Accuracy: 0.7000540443163394\n",
      "Sample IFCB5_2011_188_161249 (447/678) -> Accuracy: 0.7215422276621787\n",
      "Sample IFCB5_2011_188_163602 (448/678) -> Accuracy: 0.730241935483871\n",
      "Sample IFCB5_2011_188_165915 (449/678) -> Accuracy: 0.7459931020490972\n",
      "Sample IFCB5_2011_208_191537 (450/678) -> Accuracy: 0.77578125\n",
      "Sample IFCB5_2011_208_193851 (451/678) -> Accuracy: 0.7752100840336135\n",
      "Sample IFCB5_2011_304_182427 (452/678) -> Accuracy: 0.8940895741556535\n",
      "Sample IFCB5_2011_304_184510 (453/678) -> Accuracy: 0.8857841425078589\n",
      "Sample IFCB5_2011_304_190824 (454/678) -> Accuracy: 0.8758815232722144\n",
      "Sample IFCB5_2011_318_030612 (455/678) -> Accuracy: 0.8896551724137931\n",
      "Sample IFCB5_2011_318_031734 (456/678) -> Accuracy: 0.8740568775391758\n",
      "Sample IFCB5_2011_318_032910 (457/678) -> Accuracy: 0.8914233576642335\n",
      "Sample IFCB5_2011_318_034046 (458/678) -> Accuracy: 0.9007523148148148\n",
      "Sample IFCB5_2011_318_035238 (459/678) -> Accuracy: 0.8878944889307584\n",
      "Sample IFCB5_2011_326_161051 (460/678) -> Accuracy: 0.9198286413708691\n",
      "Sample IFCB5_2011_326_162047 (461/678) -> Accuracy: 0.9096787229743383\n",
      "Sample IFCB5_2011_326_164359 (462/678) -> Accuracy: 0.9118528882670398\n",
      "Sample IFCB5_2011_326_170712 (463/678) -> Accuracy: 0.9060322516424447\n",
      "Sample IFCB5_2011_326_173024 (464/678) -> Accuracy: 0.9075766808856388\n",
      "Sample IFCB5_2011_326_175336 (465/678) -> Accuracy: 0.9020053752325822\n",
      "Sample IFCB5_2011_326_181649 (466/678) -> Accuracy: 0.904016913319239\n",
      "Sample IFCB5_2011_326_184000 (467/678) -> Accuracy: 0.8990299823633157\n",
      "Sample IFCB5_2011_326_190312 (468/678) -> Accuracy: 0.8941932522377782\n",
      "Sample IFCB5_2011_326_192626 (469/678) -> Accuracy: 0.8996647509578544\n",
      "Sample IFCB5_2011_326_194937 (470/678) -> Accuracy: 0.8973723351512147\n",
      "Sample IFCB5_2011_326_201250 (471/678) -> Accuracy: 0.898503740648379\n",
      "Sample IFCB5_2011_346_194109 (472/678) -> Accuracy: 0.9431632197589644\n",
      "Sample IFCB5_2011_346_200422 (473/678) -> Accuracy: 0.9414932324261388\n",
      "Sample IFCB5_2011_361_061143 (474/678) -> Accuracy: 0.9297297297297298\n",
      "Sample IFCB5_2011_361_063456 (475/678) -> Accuracy: 0.9238247011952191\n",
      "Sample IFCB5_2011_361_065809 (476/678) -> Accuracy: 0.9230033476805356\n",
      "Sample IFCB5_2012_009_212600 (477/678) -> Accuracy: 0.9046597777344512\n",
      "Sample IFCB5_2012_009_214911 (478/678) -> Accuracy: 0.896689497716895\n",
      "Sample IFCB5_2012_009_221223 (479/678) -> Accuracy: 0.8889753062414933\n",
      "Sample IFCB5_2012_029_070125 (480/678) -> Accuracy: 0.8341985723556132\n",
      "Sample IFCB5_2012_029_071408 (481/678) -> Accuracy: 0.8366849482023156\n",
      "Sample IFCB5_2012_029_072437 (482/678) -> Accuracy: 0.8289436420277542\n",
      "Sample IFCB5_2012_029_074009 (483/678) -> Accuracy: 0.8412852969814996\n",
      "Sample IFCB5_2012_029_074749 (484/678) -> Accuracy: 0.8286458333333333\n",
      "Sample IFCB5_2012_029_080451 (485/678) -> Accuracy: 0.8110119047619048\n",
      "Sample IFCB5_2012_039_201905 (486/678) -> Accuracy: 0.8125842696629213\n",
      "Sample IFCB5_2012_039_204218 (487/678) -> Accuracy: 0.7950116913484022\n",
      "Sample IFCB5_2012_039_210530 (488/678) -> Accuracy: 0.7768361581920904\n",
      "Sample IFCB5_2012_046_181222 (489/678) -> Accuracy: 0.8517932489451476\n",
      "Sample IFCB5_2012_046_183534 (490/678) -> Accuracy: 0.823439099283521\n",
      "Sample IFCB5_2012_046_185848 (491/678) -> Accuracy: 0.819672131147541\n",
      "Sample IFCB5_2012_052_021037 (492/678) -> Accuracy: 0.8015075376884422\n",
      "Sample IFCB5_2012_052_023350 (493/678) -> Accuracy: 0.8096692111959287\n",
      "Sample IFCB5_2012_052_025703 (494/678) -> Accuracy: 0.7753876938469234\n",
      "Sample IFCB5_2012_066_155217 (495/678) -> Accuracy: 0.8979064357715172\n",
      "Sample IFCB5_2012_066_161529 (496/678) -> Accuracy: 0.9062083666400214\n",
      "Sample IFCB5_2012_066_163841 (497/678) -> Accuracy: 0.892684238838085\n",
      "Sample IFCB5_2012_066_170154 (498/678) -> Accuracy: 0.9055851063829787\n",
      "Sample IFCB5_2012_066_172506 (499/678) -> Accuracy: 0.9072994726616709\n",
      "Sample IFCB5_2012_066_174817 (500/678) -> Accuracy: 0.9118741058655222\n",
      "Sample IFCB5_2012_066_181129 (501/678) -> Accuracy: 0.906034720308625\n",
      "Sample IFCB5_2012_066_183442 (502/678) -> Accuracy: 0.8983286908077994\n",
      "Sample IFCB5_2012_066_185753 (503/678) -> Accuracy: 0.8930205949656751\n",
      "Sample IFCB5_2012_066_192107 (504/678) -> Accuracy: 0.9070288434612154\n",
      "Sample IFCB5_2012_066_194419 (505/678) -> Accuracy: 0.9079977628635347\n",
      "Sample IFCB5_2012_066_200731 (506/678) -> Accuracy: 0.9005509641873278\n",
      "Sample IFCB5_2012_066_203043 (507/678) -> Accuracy: 0.8811935395565289\n",
      "Sample IFCB5_2012_066_205355 (508/678) -> Accuracy: 0.8959552953698776\n",
      "Sample IFCB5_2012_078_060738 (509/678) -> Accuracy: 0.9135609756097561\n",
      "Sample IFCB5_2012_078_063051 (510/678) -> Accuracy: 0.9051848791571989\n",
      "Sample IFCB5_2012_078_065403 (511/678) -> Accuracy: 0.9066035795103888\n",
      "Sample IFCB5_2012_104_055519 (512/678) -> Accuracy: 0.8068376068376069\n",
      "Sample IFCB5_2012_104_061830 (513/678) -> Accuracy: 0.8105001553277416\n",
      "Sample IFCB5_2012_104_064143 (514/678) -> Accuracy: 0.8055997454661151\n",
      "Sample IFCB5_2012_192_163711 (515/678) -> Accuracy: 0.9016799292661362\n",
      "Sample IFCB5_2012_192_165320 (516/678) -> Accuracy: 0.8932752179327522\n",
      "Sample IFCB5_2012_192_171322 (517/678) -> Accuracy: 0.9696969696969697\n",
      "Sample IFCB5_2012_192_171632 (518/678) -> Accuracy: 0.8873483535528596\n",
      "Sample IFCB5_2012_192_173944 (519/678) -> Accuracy: 0.8926129507976941\n",
      "Sample IFCB5_2012_192_180257 (520/678) -> Accuracy: 0.9138519924098671\n",
      "Sample IFCB5_2012_192_182636 (521/678) -> Accuracy: 0.9110816191108162\n",
      "Sample IFCB5_2012_192_185000 (522/678) -> Accuracy: 0.9056179775280899\n",
      "Sample IFCB5_2012_192_191234 (523/678) -> Accuracy: 0.9045719982908418\n",
      "Sample IFCB5_2012_192_193545 (524/678) -> Accuracy: 0.8992042440318302\n",
      "Sample IFCB5_2012_192_195858 (525/678) -> Accuracy: 0.8982446516730663\n",
      "Sample IFCB5_2012_192_202210 (526/678) -> Accuracy: 0.902295166046929\n",
      "Sample IFCB5_2012_192_204522 (527/678) -> Accuracy: 0.9133112269066793\n",
      "Sample IFCB5_2012_198_225348 (528/678) -> Accuracy: 0.8585263157894737\n",
      "Sample IFCB5_2012_198_231159 (529/678) -> Accuracy: 0.861646234676007\n",
      "Sample IFCB5_2012_198_231629 (530/678) -> Accuracy: 0.8491003271537623\n",
      "Sample IFCB5_2012_198_233432 (531/678) -> Accuracy: 0.8624161073825504\n",
      "Sample IFCB5_2012_198_234009 (532/678) -> Accuracy: 0.8422210005497526\n",
      "Sample IFCB5_2012_198_235748 (533/678) -> Accuracy: 0.8677685950413223\n",
      "Sample IFCB5_2012_221_101206 (534/678) -> Accuracy: 0.7529725794710022\n",
      "Sample IFCB5_2012_221_103446 (535/678) -> Accuracy: 0.7348325134691965\n",
      "Sample IFCB5_2012_221_105833 (536/678) -> Accuracy: 0.7537105751391465\n",
      "Sample IFCB5_2012_233_152139 (537/678) -> Accuracy: 0.867231638418079\n",
      "Sample IFCB5_2012_233_154446 (538/678) -> Accuracy: 0.8625385872525876\n",
      "Sample IFCB5_2012_233_160757 (539/678) -> Accuracy: 0.8730928020104111\n",
      "Sample IFCB5_2012_242_001737 (540/678) -> Accuracy: 0.9034879783271249\n",
      "Sample IFCB5_2012_242_004042 (541/678) -> Accuracy: 0.8991430454845089\n",
      "Sample IFCB5_2012_242_010357 (542/678) -> Accuracy: 0.8900117904665656\n",
      "Sample IFCB5_2013_067_224713 (543/678) -> Accuracy: 0.8849400266548201\n",
      "Sample IFCB5_2013_067_230254 (544/678) -> Accuracy: 0.8871369294605809\n",
      "Sample IFCB5_2013_067_231024 (545/678) -> Accuracy: 0.8944591029023746\n",
      "Sample IFCB5_2013_067_232646 (546/678) -> Accuracy: 0.8664627930682977\n",
      "Sample IFCB5_2013_067_233356 (547/678) -> Accuracy: 0.890380805635043\n",
      "Sample IFCB5_2013_067_235006 (548/678) -> Accuracy: 0.8980827447023209\n",
      "Sample IFCB5_2013_087_075228 (549/678) -> Accuracy: 0.91597018296815\n",
      "Sample IFCB5_2013_087_081527 (550/678) -> Accuracy: 0.9119003690036901\n",
      "Sample IFCB5_2013_087_083508 (551/678) -> Accuracy: 0.84375\n",
      "Sample IFCB5_2013_087_083850 (552/678) -> Accuracy: 0.9019381633594832\n",
      "Sample IFCB5_2013_094_130629 (553/678) -> Accuracy: 0.907103825136612\n",
      "Sample IFCB5_2013_094_132943 (554/678) -> Accuracy: 0.9083044982698962\n",
      "Sample IFCB5_2013_094_135255 (555/678) -> Accuracy: 0.8925193465176269\n",
      "Sample IFCB5_2013_113_095948 (556/678) -> Accuracy: 0.8193430656934306\n",
      "Sample IFCB5_2013_113_102301 (557/678) -> Accuracy: 0.7877145438121048\n",
      "Sample IFCB5_2013_113_104613 (558/678) -> Accuracy: 0.8158362989323843\n",
      "Sample IFCB5_2013_119_175520 (559/678) -> Accuracy: 0.919218372280419\n",
      "Sample IFCB5_2013_119_181832 (560/678) -> Accuracy: 0.880615234375\n",
      "Sample IFCB5_2013_119_184144 (561/678) -> Accuracy: 0.8742569730224051\n",
      "Sample IFCB5_2013_120_001239 (562/678) -> Accuracy: 0.7928631168107053\n",
      "Sample IFCB5_2013_127_193749 (563/678) -> Accuracy: 0.9232908458864426\n",
      "Sample IFCB5_2013_127_200102 (564/678) -> Accuracy: 0.9230264596544938\n",
      "Sample IFCB5_2013_127_202414 (565/678) -> Accuracy: 0.920851654709558\n",
      "Sample IFCB5_2013_144_010916 (566/678) -> Accuracy: 0.8441933788754598\n",
      "Sample IFCB5_2013_144_013228 (567/678) -> Accuracy: 0.8437668645439828\n",
      "Sample IFCB5_2013_144_015540 (568/678) -> Accuracy: 0.8746766312158666\n",
      "Sample IFCB5_2013_153_131718 (569/678) -> Accuracy: 0.9585318737926594\n",
      "Sample IFCB5_2013_153_134030 (570/678) -> Accuracy: 0.960232293902285\n",
      "Sample IFCB5_2013_153_140343 (571/678) -> Accuracy: 0.9552294490734983\n",
      "Sample IFCB5_2013_167_150504 (572/678) -> Accuracy: 0.931879337204362\n",
      "Sample IFCB5_2013_167_152816 (573/678) -> Accuracy: 0.9229460003094538\n",
      "Sample IFCB5_2013_167_155128 (574/678) -> Accuracy: 0.9259201979585524\n",
      "Sample IFCB5_2013_172_185238 (575/678) -> Accuracy: 0.8122456888199961\n",
      "Sample IFCB5_2013_172_191227 (576/678) -> Accuracy: 0.7864077669902912\n",
      "Sample IFCB5_2013_172_191550 (577/678) -> Accuracy: 0.809807425054596\n",
      "Sample IFCB5_2013_172_193903 (578/678) -> Accuracy: 0.8070996684220791\n",
      "Sample IFCB5_2013_184_211057 (579/678) -> Accuracy: 0.8719611021069692\n",
      "Sample IFCB5_2013_184_213410 (580/678) -> Accuracy: 0.8681318681318682\n",
      "Sample IFCB5_2013_184_215401 (581/678) -> Accuracy: 0.8737864077669902\n",
      "Sample IFCB5_2013_184_215723 (582/678) -> Accuracy: 0.869198312236287\n",
      "Sample IFCB5_2013_194_135924 (583/678) -> Accuracy: 0.8771433327784252\n",
      "Sample IFCB5_2013_194_142018 (584/678) -> Accuracy: 0.8710732984293194\n",
      "Sample IFCB5_2013_194_143924 (585/678) -> Accuracy: 0.8870967741935484\n",
      "Sample IFCB5_2013_194_144330 (586/678) -> Accuracy: 0.8800064082024992\n",
      "Sample IFCB5_2013_198_193912 (587/678) -> Accuracy: 0.8290797724952603\n",
      "Sample IFCB5_2013_198_200218 (588/678) -> Accuracy: 0.8208316065754938\n",
      "Sample IFCB5_2013_198_202531 (589/678) -> Accuracy: 0.8631899762952929\n",
      "Sample IFCB5_2013_213_160322 (590/678) -> Accuracy: 0.8332007425086184\n",
      "Sample IFCB5_2013_213_162257 (591/678) -> Accuracy: 0.819047619047619\n",
      "Sample IFCB5_2013_213_162635 (592/678) -> Accuracy: 0.8253401126837983\n",
      "Sample IFCB5_2013_213_164516 (593/678) -> Accuracy: 0.8085867620751341\n",
      "Sample IFCB5_2013_213_164947 (594/678) -> Accuracy: 0.8277148567621586\n",
      "Sample IFCB5_2013_225_161158 (595/678) -> Accuracy: 0.9023994436072794\n",
      "Sample IFCB5_2013_225_163107 (596/678) -> Accuracy: 0.8791946308724832\n",
      "Sample IFCB5_2013_225_163510 (597/678) -> Accuracy: 0.9049172687019342\n",
      "Sample IFCB5_2013_225_165416 (598/678) -> Accuracy: 0.919917864476386\n",
      "Sample IFCB5_2013_225_165823 (599/678) -> Accuracy: 0.911922025436824\n",
      "Sample IFCB5_2013_225_171634 (600/678) -> Accuracy: 0.9048140043763676\n",
      "Sample IFCB5_2013_235_180640 (601/678) -> Accuracy: 0.8656940893367424\n",
      "Sample IFCB5_2013_235_182200 (602/678) -> Accuracy: 0.8678555098308185\n",
      "Sample IFCB5_2013_235_182951 (603/678) -> Accuracy: 0.8709428129829985\n",
      "Sample IFCB5_2013_235_184442 (604/678) -> Accuracy: 0.8646431501230517\n",
      "Sample IFCB5_2013_235_185303 (605/678) -> Accuracy: 0.8534720097889262\n",
      "Sample IFCB5_2013_235_190731 (606/678) -> Accuracy: 0.8690702087286527\n",
      "Sample IFCB5_2013_250_011155 (607/678) -> Accuracy: 0.8627362055933484\n",
      "Sample IFCB5_2013_250_013035 (608/678) -> Accuracy: 0.8322784810126582\n",
      "Sample IFCB5_2013_250_013451 (609/678) -> Accuracy: 0.8658355595132087\n",
      "Sample IFCB5_2013_250_015403 (610/678) -> Accuracy: 0.8811881188118812\n",
      "Sample IFCB5_2013_250_015822 (611/678) -> Accuracy: 0.8629093678598629\n",
      "Sample IFCB5_2013_250_021701 (612/678) -> Accuracy: 0.8830845771144279\n",
      "Sample IFCB5_2013_262_121153 (613/678) -> Accuracy: 0.8545597484276729\n",
      "Sample IFCB5_2013_262_123509 (614/678) -> Accuracy: 0.8181378476420798\n",
      "Sample IFCB5_2013_262_125028 (615/678) -> Accuracy: 0.8446511627906976\n",
      "Sample IFCB5_2013_262_125811 (616/678) -> Accuracy: 0.8473216222077641\n",
      "Sample IFCB5_2013_262_131609 (617/678) -> Accuracy: 0.8270142180094787\n",
      "Sample IFCB5_2013_325_221717 (618/678) -> Accuracy: 0.796\n",
      "Sample IFCB5_2013_325_224041 (619/678) -> Accuracy: 0.8024143440440262\n",
      "Sample IFCB5_2013_325_230315 (620/678) -> Accuracy: 0.8146937370956642\n",
      "Sample IFCB5_2013_343_170548 (621/678) -> Accuracy: 0.89337795840306\n",
      "Sample IFCB5_2013_343_172900 (622/678) -> Accuracy: 0.8923712342079689\n",
      "Sample IFCB5_2013_343_175211 (623/678) -> Accuracy: 0.8968929804372843\n",
      "Sample IFCB5_2013_352_090027 (624/678) -> Accuracy: 0.8845628415300546\n",
      "Sample IFCB5_2013_352_091741 (625/678) -> Accuracy: 0.8890489913544669\n",
      "Sample IFCB5_2013_352_092339 (626/678) -> Accuracy: 0.8706443914081146\n",
      "Sample IFCB5_2013_352_094025 (627/678) -> Accuracy: 0.8655980271270037\n",
      "Sample IFCB5_2013_352_094651 (628/678) -> Accuracy: 0.8670140462165836\n",
      "Sample IFCB5_2013_352_100428 (629/678) -> Accuracy: 0.8633540372670807\n",
      "Sample IFCB5_2014_002_201555 (630/678) -> Accuracy: 0.7561737155833097\n",
      "Sample IFCB5_2014_002_203908 (631/678) -> Accuracy: 0.756797583081571\n",
      "Sample IFCB5_2014_002_210221 (632/678) -> Accuracy: 0.7815384615384615\n",
      "Sample IFCB5_2014_016_191101 (633/678) -> Accuracy: 0.8481958762886598\n",
      "Sample IFCB5_2014_016_193413 (634/678) -> Accuracy: 0.8410509885535901\n",
      "Sample IFCB5_2014_016_195342 (635/678) -> Accuracy: 0.816\n",
      "Sample IFCB5_2014_016_195725 (636/678) -> Accuracy: 0.8391891891891892\n",
      "Sample IFCB5_2014_038_210605 (637/678) -> Accuracy: 0.9207606973058637\n",
      "Sample IFCB5_2014_038_212917 (638/678) -> Accuracy: 0.9028841820639928\n",
      "Sample IFCB5_2014_038_215230 (639/678) -> Accuracy: 0.9100661949326638\n",
      "Sample IFCB5_2014_057_184322 (640/678) -> Accuracy: 0.9263080324244657\n",
      "Sample IFCB5_2014_057_190635 (641/678) -> Accuracy: 0.9285226496699295\n",
      "Sample IFCB5_2014_057_192946 (642/678) -> Accuracy: 0.929749265868534\n",
      "Sample IFCB5_2014_060_060318 (643/678) -> Accuracy: 0.9174177321316286\n",
      "Sample IFCB5_2014_060_062630 (644/678) -> Accuracy: 0.9267437061494016\n",
      "Sample IFCB5_2014_060_064942 (645/678) -> Accuracy: 0.9303313508920985\n",
      "Sample IFCB5_2014_078_000841 (646/678) -> Accuracy: 0.9550953454992823\n",
      "Sample IFCB5_2014_078_003154 (647/678) -> Accuracy: 0.9605444860919313\n",
      "Sample IFCB5_2014_078_005507 (648/678) -> Accuracy: 0.9424184261036468\n",
      "Sample IFCB5_2014_092_181128 (649/678) -> Accuracy: 0.9748238977302374\n",
      "Sample IFCB5_2014_092_183440 (650/678) -> Accuracy: 0.9725782414307005\n",
      "Sample IFCB5_2014_092_185752 (651/678) -> Accuracy: 0.9692121943857531\n",
      "Sample IFCB5_2014_108_210902 (652/678) -> Accuracy: 0.9528824833702882\n",
      "Sample IFCB5_2014_108_213216 (653/678) -> Accuracy: 0.9587485515643105\n",
      "Sample IFCB5_2014_108_215526 (654/678) -> Accuracy: 0.9465832531280077\n",
      "Sample IFCB5_2014_134_011015 (655/678) -> Accuracy: 0.9716383049716383\n",
      "Sample IFCB5_2014_134_013328 (656/678) -> Accuracy: 0.9633450395083406\n",
      "Sample IFCB5_2014_134_015639 (657/678) -> Accuracy: 0.9721324267082823\n",
      "Sample IFCB5_2014_238_212841 (658/678) -> Accuracy: 0.8867878192534381\n",
      "Sample IFCB5_2014_238_215151 (659/678) -> Accuracy: 0.8896214896214896\n",
      "Sample IFCB5_2014_238_221441 (660/678) -> Accuracy: 0.8804733727810651\n",
      "Sample IFCB5_2014_248_001737 (661/678) -> Accuracy: 0.8923142795975166\n",
      "Sample IFCB5_2014_248_004113 (662/678) -> Accuracy: 0.8931623931623932\n",
      "Sample IFCB5_2014_248_010423 (663/678) -> Accuracy: 0.8900819992190551\n",
      "Sample IFCB5_2014_259_113906 (664/678) -> Accuracy: 0.8461162677088422\n",
      "Sample IFCB5_2014_259_120213 (665/678) -> Accuracy: 0.8349900596421471\n",
      "Sample IFCB5_2014_259_122510 (666/678) -> Accuracy: 0.8452354133813922\n",
      "Sample IFCB5_2014_315_135823 (667/678) -> Accuracy: 0.8994732881866064\n",
      "Sample IFCB5_2014_315_142115 (668/678) -> Accuracy: 0.8970718722271517\n",
      "Sample IFCB5_2014_315_144452 (669/678) -> Accuracy: 0.8945682248434542\n",
      "Sample IFCB5_2014_328_152205 (670/678) -> Accuracy: 0.9267175572519084\n",
      "Sample IFCB5_2014_328_154517 (671/678) -> Accuracy: 0.9317399617590822\n",
      "Sample IFCB5_2014_328_160829 (672/678) -> Accuracy: 0.9228836490143023\n",
      "Sample IFCB5_2014_341_020748 (673/678) -> Accuracy: 0.9166101694915254\n",
      "Sample IFCB5_2014_341_023101 (674/678) -> Accuracy: 0.920303605313093\n",
      "Sample IFCB5_2014_341_025413 (675/678) -> Accuracy: 0.914500683994528\n",
      "Sample IFCB5_2014_353_200517 (676/678) -> Accuracy: 0.9363151113565249\n",
      "Sample IFCB5_2014_353_202828 (677/678) -> Accuracy: 0.9222285605628446\n",
      "Sample IFCB5_2014_353_205141 (678/678) -> Accuracy: 0.9113997673516867\n",
      "Predicting also train and val samples...\n",
      "Sample IFCB1_2008_253_060842 (1/286) -> Accuracy: 0.9133154602323503\n",
      "Sample IFCB1_2008_100_194609 (2/286) -> Accuracy: 0.9618511569731082\n",
      "Sample IFCB1_2007_101_180512 (3/286) -> Accuracy: 0.8954128440366973\n",
      "Sample IFCB1_2007_101_004828 (4/286) -> Accuracy: 0.905511811023622\n",
      "Sample IFCB1_2007_058_004930 (5/286) -> Accuracy: 0.9019908116385911\n",
      "Sample IFCB1_2008_218_192834 (6/286) -> Accuracy: 0.936738397718434\n",
      "Sample IFCB1_2008_235_205819 (7/286) -> Accuracy: 0.7949820788530466\n",
      "Sample IFCB1_2008_102_144258 (8/286) -> Accuracy: 0.9842027920646583\n",
      "Sample IFCB1_2008_235_203502 (9/286) -> Accuracy: 0.7982608695652174\n",
      "Sample IFCB1_2008_072_183155 (10/286) -> Accuracy: 0.9517903112567283\n",
      "Sample IFCB1_2008_114_173253 (11/286) -> Accuracy: 0.8776346604215457\n",
      "Sample IFCB1_2006_208_184849 (12/286) -> Accuracy: 0.888745148771022\n",
      "Sample IFCB1_2006_354_185546 (13/286) -> Accuracy: 0.8720516962843295\n",
      "Sample IFCB1_2007_347_004316 (14/286) -> Accuracy: 0.9578328021573915\n",
      "Sample IFCB1_2008_199_143321 (15/286) -> Accuracy: 0.9197364480383349\n",
      "Sample IFCB1_2007_309_185604 (16/286) -> Accuracy: 0.9231923942957218\n",
      "Sample IFCB1_2007_072_001941 (17/286) -> Accuracy: 0.9210977701543739\n",
      "Sample IFCB1_2008_085_182934 (18/286) -> Accuracy: 0.9512073686830969\n",
      "Sample IFCB1_2006_295_005130 (19/286) -> Accuracy: 0.93929173693086\n",
      "Sample IFCB1_2006_361_000315 (20/286) -> Accuracy: 0.9423917361938816\n",
      "Sample IFCB1_2008_218_190452 (21/286) -> Accuracy: 0.9195371367061357\n",
      "Sample IFCB1_2008_085_005225 (22/286) -> Accuracy: 0.9330323551542513\n",
      "Sample IFCB1_2006_198_225705 (23/286) -> Accuracy: 0.8911404335532517\n",
      "Sample IFCB1_2006_191_003426 (24/286) -> Accuracy: 0.7867620751341682\n",
      "Sample IFCB1_2007_318_000513 (25/286) -> Accuracy: 0.9564288063137126\n",
      "Sample IFCB1_2007_158_140339 (26/286) -> Accuracy: 0.9064130101258054\n",
      "Sample IFCB1_2007_183_004712 (27/286) -> Accuracy: 0.8991198375084631\n",
      "Sample IFCB1_2007_303_204500 (28/286) -> Accuracy: 0.875290601129193\n",
      "Sample IFCB1_2006_205_030430 (29/286) -> Accuracy: 0.8755980861244019\n",
      "Sample IFCB1_2007_129_182605 (30/286) -> Accuracy: 0.8798955613577023\n",
      "Sample IFCB1_2006_191_001242 (31/286) -> Accuracy: 0.7799660441426146\n",
      "Sample IFCB1_2006_238_183657 (32/286) -> Accuracy: 0.8687150837988827\n",
      "Sample IFCB1_2007_283_004729 (33/286) -> Accuracy: 0.9356649395509499\n",
      "Sample IFCB1_2007_343_211257 (34/286) -> Accuracy: 0.9598145285935085\n",
      "Sample IFCB1_2008_160_000819 (35/286) -> Accuracy: 0.8368533823079277\n",
      "Sample IFCB1_2008_218_003836 (36/286) -> Accuracy: 0.9385414412464834\n",
      "Sample IFCB1_2006_172_174135 (37/286) -> Accuracy: 0.8808146844355041\n",
      "Sample IFCB1_2007_010_003906 (38/286) -> Accuracy: 0.9209690230341541\n",
      "Sample IFCB1_2007_156_004728 (39/286) -> Accuracy: 0.9404978749241044\n",
      "Sample IFCB1_2006_171_003522 (40/286) -> Accuracy: 0.8253221517692779\n",
      "Sample IFCB1_2007_101_163817 (41/286) -> Accuracy: 0.8786206896551724\n",
      "Sample IFCB1_2006_238_185830 (42/286) -> Accuracy: 0.8813905930470347\n",
      "Sample IFCB1_2008_237_195716 (43/286) -> Accuracy: 0.8954186413902053\n",
      "Sample IFCB1_2008_001_004404 (44/286) -> Accuracy: 0.8722451790633609\n",
      "Sample IFCB1_2007_146_004733 (45/286) -> Accuracy: 0.9161290322580645\n",
      "Sample IFCB1_2007_114_180853 (46/286) -> Accuracy: 0.9294320137693631\n",
      "Sample IFCB1_2008_319_005351 (47/286) -> Accuracy: 0.9505267118133935\n",
      "Sample IFCB1_2007_032_183908 (48/286) -> Accuracy: 0.9039167686658507\n",
      "Sample IFCB1_2006_270_172914 (49/286) -> Accuracy: 0.9062618595825427\n",
      "Sample IFCB1_2007_361_001447 (50/286) -> Accuracy: 0.894919168591224\n",
      "Sample IFCB1_2008_072_005515 (51/286) -> Accuracy: 0.9497844827586207\n",
      "Sample IFCB1_2008_073_152345 (52/286) -> Accuracy: 0.9624678663239075\n",
      "Sample IFCB1_2008_327_001031 (53/286) -> Accuracy: 0.942262504196039\n",
      "Sample IFCB1_2007_101_182657 (54/286) -> Accuracy: 0.9237012987012987\n",
      "Sample IFCB1_2008_248_002039 (55/286) -> Accuracy: 0.8836192398250925\n",
      "Sample IFCB1_2007_318_184658 (56/286) -> Accuracy: 0.9504600141542817\n",
      "Sample IFCB1_2008_001_000039 (57/286) -> Accuracy: 0.8792510508215514\n",
      "Sample IFCB1_2008_253_054558 (58/286) -> Accuracy: 0.9215134459036898\n",
      "Sample IFCB1_2008_116_215800 (59/286) -> Accuracy: 0.931958762886598\n",
      "Sample IFCB1_2007_168_005953 (60/286) -> Accuracy: 0.8499475341028332\n",
      "Sample IFCB1_2006_237_000054 (61/286) -> Accuracy: 0.8434504792332268\n",
      "Sample IFCB1_2007_183_002529 (62/286) -> Accuracy: 0.9139590854392299\n",
      "Sample IFCB1_2008_194_002638 (63/286) -> Accuracy: 0.9328739719937764\n",
      "Sample IFCB1_2007_129_180420 (64/286) -> Accuracy: 0.8840125391849529\n",
      "Sample IFCB1_2008_318_195140 (65/286) -> Accuracy: 0.915803352797137\n",
      "Sample IFCB1_2008_213_194911 (66/286) -> Accuracy: 0.9283797729618163\n",
      "Sample IFCB1_2006_205_001923 (67/286) -> Accuracy: 0.9099271751628977\n",
      "Sample IFCB1_2008_160_005420 (68/286) -> Accuracy: 0.8900699300699301\n",
      "Sample IFCB1_2007_024_215846 (69/286) -> Accuracy: 0.9456572224802601\n",
      "Sample IFCB1_2006_237_002246 (70/286) -> Accuracy: 0.8677966101694915\n",
      "Sample IFCB1_2008_319_000728 (71/286) -> Accuracy: 0.954562957258375\n",
      "Sample IFCB1_2007_027_000718 (72/286) -> Accuracy: 0.9605621033544878\n",
      "Sample IFCB1_2007_141_113857 (73/286) -> Accuracy: 0.8421348314606741\n",
      "Sample IFCB1_2007_089_002606 (74/286) -> Accuracy: 0.8899881282152751\n",
      "Sample IFCB1_2007_168_100326 (75/286) -> Accuracy: 0.8924358084663429\n",
      "Sample IFCB1_2008_060_001013 (76/286) -> Accuracy: 0.8739585558641316\n",
      "Sample IFCB1_2008_347_184742 (77/286) -> Accuracy: 0.9103690685413005\n",
      "Sample IFCB1_2008_114_004221 (78/286) -> Accuracy: 0.9230512991339107\n",
      "Sample IFCB1_2007_059_212939 (79/286) -> Accuracy: 0.9248334919124643\n",
      "Sample IFCB1_2007_290_162103 (80/286) -> Accuracy: 0.9149047702254063\n",
      "Sample IFCB1_2008_073_150206 (81/286) -> Accuracy: 0.9640479360852197\n",
      "Sample IFCB1_2007_101_161630 (82/286) -> Accuracy: 0.9049034175334324\n",
      "Sample IFCB1_2006_295_000750 (83/286) -> Accuracy: 0.9382780082987552\n",
      "Sample IFCB1_2007_129_002550 (84/286) -> Accuracy: 0.9068702290076336\n",
      "Sample IFCB1_2008_095_184031 (85/286) -> Accuracy: 0.8903083700440528\n",
      "Sample IFCB1_2007_304_000011 (86/286) -> Accuracy: 0.8985762711864407\n",
      "Sample IFCB1_2008_114_175618 (87/286) -> Accuracy: 0.8504297994269341\n",
      "Sample IFCB1_2008_160_003109 (88/286) -> Accuracy: 0.858338141950375\n",
      "Sample IFCB1_2007_114_005205 (89/286) -> Accuracy: 0.931\n",
      "Sample IFCB1_2006_272_005156 (90/286) -> Accuracy: 0.9651101783840503\n",
      "Sample IFCB1_2006_171_001337 (91/286) -> Accuracy: 0.8518987341772152\n",
      "Sample IFCB1_2006_172_171947 (92/286) -> Accuracy: 0.8880874825500232\n",
      "Sample IFCB1_2006_158_004408 (93/286) -> Accuracy: 0.9168502202643172\n",
      "Sample IFCB1_2008_123_174149 (94/286) -> Accuracy: 0.8270676691729323\n",
      "Sample IFCB1_2007_283_002541 (95/286) -> Accuracy: 0.9359272573482766\n",
      "Sample IFCB1_2008_218_184206 (96/286) -> Accuracy: 0.9054913294797687\n",
      "Sample IFCB1_2008_200_003651 (97/286) -> Accuracy: 0.9069299689157068\n",
      "Sample IFCB1_2006_354_191732 (98/286) -> Accuracy: 0.8659378596087457\n",
      "Sample IFCB1_2008_132_005343 (99/286) -> Accuracy: 0.8104852993269571\n",
      "Sample IFCB1_2007_045_005405 (100/286) -> Accuracy: 0.8816425120772947\n",
      "Sample IFCB1_2008_144_154542 (101/286) -> Accuracy: 0.8662337662337662\n",
      "Sample IFCB1_2006_307_222019 (102/286) -> Accuracy: 0.8403547671840355\n",
      "Sample IFCB1_2008_123_171849 (103/286) -> Accuracy: 0.9135802469135802\n",
      "Sample IFCB1_2008_132_031300 (104/286) -> Accuracy: 0.8604823747680891\n",
      "Sample IFCB1_2007_183_101331 (105/286) -> Accuracy: 0.8890063424947146\n",
      "Sample IFCB1_2008_114_001918 (106/286) -> Accuracy: 0.9052333804809052\n",
      "Sample IFCB1_2008_236_000420 (107/286) -> Accuracy: 0.8234106962663976\n",
      "Sample IFCB1_2006_237_004422 (108/286) -> Accuracy: 0.8594771241830066\n",
      "Sample IFCB1_2006_238_192014 (109/286) -> Accuracy: 0.893305439330544\n",
      "Sample IFCB1_2008_213_192735 (110/286) -> Accuracy: 0.9344507632445376\n",
      "Sample IFCB1_2008_318_190514 (111/286) -> Accuracy: 0.9118711545421643\n",
      "Sample IFCB1_2007_304_002152 (112/286) -> Accuracy: 0.9026246311539059\n",
      "Sample IFCB1_2007_146_002550 (113/286) -> Accuracy: 0.9159109645507008\n",
      "Sample IFCB1_2007_123_200551 (114/286) -> Accuracy: 0.8950437317784257\n",
      "Sample IFCB1_2006_208_190943 (115/286) -> Accuracy: 0.8359281437125748\n",
      "Sample IFCB1_2008_347_182427 (116/286) -> Accuracy: 0.9258087201125176\n",
      "Sample IFCB1_2008_218_181829 (117/286) -> Accuracy: 0.9050903119868637\n",
      "Sample IFCB1_2008_236_002732 (118/286) -> Accuracy: 0.8556597454789016\n",
      "Sample IFCB1_2007_361_003629 (119/286) -> Accuracy: 0.9033742331288344\n",
      "Sample IFCB1_2006_191_005610 (120/286) -> Accuracy: 0.8089926803764378\n",
      "Sample IFCB1_2007_059_221306 (121/286) -> Accuracy: 0.9425612052730696\n",
      "Sample IFCB1_2006_272_003012 (122/286) -> Accuracy: 0.9654775604142692\n",
      "Sample IFCB1_2007_114_183038 (123/286) -> Accuracy: 0.9187279151943463\n",
      "Sample IFCB1_2007_309_191746 (124/286) -> Accuracy: 0.9284634760705289\n",
      "Sample IFCB1_2008_072_003302 (125/286) -> Accuracy: 0.9465648854961832\n",
      "Sample IFCB1_2008_060_003154 (126/286) -> Accuracy: 0.9268075639599556\n",
      "Sample IFCB1_2008_248_004347 (127/286) -> Accuracy: 0.8759036144578313\n",
      "Sample IFCB1_2007_101_002642 (128/286) -> Accuracy: 0.9266347687400319\n",
      "Sample IFCB1_2006_361_002458 (129/286) -> Accuracy: 0.9470588235294117\n",
      "Sample IFCB1_2008_253_052218 (130/286) -> Accuracy: 0.9125894988066826\n",
      "Sample IFCB1_2007_343_215619 (131/286) -> Accuracy: 0.9624546866908338\n",
      "Sample IFCB1_2008_006_000316 (132/286) -> Accuracy: 0.8984263233190272\n",
      "Sample IFCB1_2006_270_164544 (133/286) -> Accuracy: 0.8975501113585747\n",
      "Sample IFCB1_2007_123_194408 (134/286) -> Accuracy: 0.9429280397022333\n",
      "Sample IFCB1_2008_102_150441 (135/286) -> Accuracy: 0.9787851719517642\n",
      "Sample IFCB1_2007_089_000422 (136/286) -> Accuracy: 0.8997594226142742\n",
      "Sample IFCB1_2008_213_201256 (137/286) -> Accuracy: 0.930041974815111\n",
      "Sample IFCB1_2006_198_223522 (138/286) -> Accuracy: 0.9148264984227129\n",
      "Sample IFCB1_2007_156_002545 (139/286) -> Accuracy: 0.9346470943092701\n",
      "Sample IFCB1_2006_354_193915 (140/286) -> Accuracy: 0.8565912851627138\n",
      "Sample IFCB1_2008_363_000153 (141/286) -> Accuracy: 0.9255568581477139\n",
      "Sample IFCB1_2007_156_000156 (142/286) -> Accuracy: 0.9230769230769231\n",
      "Sample IFCB1_2007_058_032122 (143/286) -> Accuracy: 0.9175977653631285\n",
      "Sample IFCB1_2006_280_000248 (144/286) -> Accuracy: 0.9650382032877981\n",
      "Sample IFCB1_2008_237_202052 (145/286) -> Accuracy: 0.8832142857142857\n",
      "Sample IFCB1_2007_318_182518 (146/286) -> Accuracy: 0.9575695858791582\n",
      "Sample IFCB1_2008_095_001923 (147/286) -> Accuracy: 0.9133880554769318\n",
      "Sample IFCB1_2006_272_000827 (148/286) -> Accuracy: 0.9572546658639374\n",
      "Sample IFCB1_2007_129_000404 (149/286) -> Accuracy: 0.9073482428115016\n",
      "Sample IFCB1_2008_102_152550 (150/286) -> Accuracy: 0.9820971867007673\n",
      "Sample IFCB1_2007_284_180020 (151/286) -> Accuracy: 0.9326885880077369\n",
      "Sample IFCB1_2007_027_002856 (152/286) -> Accuracy: 0.9582616179001722\n",
      "Sample IFCB1_2007_309_183421 (153/286) -> Accuracy: 0.9171958964338056\n",
      "Sample IFCB1_2008_145_001551 (154/286) -> Accuracy: 0.7494711393170143\n",
      "Sample IFCB1_2007_114_003022 (155/286) -> Accuracy: 0.9444444444444444\n",
      "Sample IFCB1_2008_085_000937 (156/286) -> Accuracy: 0.9217267552182163\n",
      "Sample IFCB1_2007_158_144708 (157/286) -> Accuracy: 0.9147760325770797\n",
      "Sample IFCB1_2008_072_185335 (158/286) -> Accuracy: 0.9578339991122947\n",
      "Sample IFCB1_2006_280_001959 (159/286) -> Accuracy: 0.9617151607963247\n",
      "Sample IFCB1_2007_343_213440 (160/286) -> Accuracy: 0.9579898963041744\n",
      "Sample IFCB1_2008_085_185117 (161/286) -> Accuracy: 0.9568703345313796\n",
      "Sample IFCB1_2007_101_000456 (162/286) -> Accuracy: 0.9239811912225705\n",
      "Sample IFCB1_2006_280_002433 (163/286) -> Accuracy: 0.9600088378258949\n",
      "Sample IFCB1_2008_132_033640 (164/286) -> Accuracy: 0.8679966044142614\n",
      "Sample IFCB1_2008_001_002223 (165/286) -> Accuracy: 0.8897133220910624\n",
      "Sample IFCB1_2008_056_170257 (166/286) -> Accuracy: 0.9511445165698668\n",
      "Sample IFCB1_2007_284_173836 (167/286) -> Accuracy: 0.9292174596932756\n",
      "Sample IFCB1_2007_168_001625 (168/286) -> Accuracy: 0.8741470811220622\n",
      "Sample IFCB1_2007_168_102508 (169/286) -> Accuracy: 0.8850029638411381\n",
      "Sample IFCB1_2007_141_111713 (170/286) -> Accuracy: 0.8396386222473179\n",
      "Sample IFCB1_2008_144_152228 (171/286) -> Accuracy: 0.855044699872286\n",
      "Sample IFCB1_2008_236_005047 (172/286) -> Accuracy: 0.8302639656230817\n",
      "Sample IFCB1_2008_056_161935 (173/286) -> Accuracy: 0.9484061781137035\n",
      "Sample IFCB1_2007_114_185222 (174/286) -> Accuracy: 0.9356814701378254\n",
      "Sample IFCB1_2008_347_180137 (175/286) -> Accuracy: 0.9193989071038251\n",
      "Sample IFCB1_2008_319_003037 (176/286) -> Accuracy: 0.9534397567464842\n",
      "Sample IFCB1_2007_113_192601 (177/286) -> Accuracy: 0.88\n",
      "Sample IFCB1_2008_043_001506 (178/286) -> Accuracy: 0.9588896697118763\n",
      "Sample IFCB1_2007_123_192222 (179/286) -> Accuracy: 0.9136490250696379\n",
      "Sample IFCB1_2008_145_003908 (180/286) -> Accuracy: 0.7434782608695653\n",
      "Sample IFCB1_2007_168_003808 (181/286) -> Accuracy: 0.8607476635514019\n",
      "Sample IFCB1_2007_113_200927 (182/286) -> Accuracy: 0.9125\n",
      "Sample IFCB1_2008_363_004832 (183/286) -> Accuracy: 0.899210686095932\n",
      "Sample IFCB1_2008_056_164116 (184/286) -> Accuracy: 0.9478795726772419\n",
      "Sample IFCB1_2008_318_192854 (185/286) -> Accuracy: 0.9056358917315536\n",
      "Sample IFCB1_2008_327_005657 (186/286) -> Accuracy: 0.9297757407104272\n",
      "Sample IFCB1_2007_303_210642 (187/286) -> Accuracy: 0.880429705744979\n",
      "Sample IFCB1_2008_363_002526 (188/286) -> Accuracy: 0.9198795180722892\n",
      "Sample IFCB1_2007_024_222028 (189/286) -> Accuracy: 0.9478021978021978\n",
      "Sample IFCB1_2008_349_180348 (190/286) -> Accuracy: 0.8674803836094158\n",
      "Sample IFCB1_2007_347_002133 (191/286) -> Accuracy: 0.9493545183714002\n",
      "Sample IFCB1_2008_327_003342 (192/286) -> Accuracy: 0.9316984858459513\n",
      "Sample IFCB1_2008_072_181011 (193/286) -> Accuracy: 0.9463176574977817\n",
      "Sample IFCB1_2007_326_031622 (194/286) -> Accuracy: 0.8670329670329671\n",
      "Sample IFCB1_2008_235_201211 (195/286) -> Accuracy: 0.7879169288860919\n",
      "Sample IFCB1_2008_043_183508 (196/286) -> Accuracy: 0.9721904761904762\n",
      "Sample IFCB1_2008_006_002456 (197/286) -> Accuracy: 0.8796778777830412\n",
      "Sample IFCB1_2006_307_213648 (198/286) -> Accuracy: 0.8389830508474576\n",
      "Sample IFCB1_2008_170_003921 (199/286) -> Accuracy: 0.800275482093664\n",
      "Sample IFCB1_2007_326_033805 (200/286) -> Accuracy: 0.8759289176090469\n",
      "Sample IFCB1_2008_237_204407 (201/286) -> Accuracy: 0.8870784388025768\n",
      "Sample IFCB1_2007_072_004124 (202/286) -> Accuracy: 0.9217511203033437\n",
      "Sample IFCB1_2007_183_103516 (203/286) -> Accuracy: 0.8832271762208068\n",
      "Sample IFCB1_2006_295_002935 (204/286) -> Accuracy: 0.9314861460957179\n",
      "Sample IFCB1_2008_144_145846 (205/286) -> Accuracy: 0.8525132275132276\n",
      "Sample IFCB1_2008_116_213522 (206/286) -> Accuracy: 0.91971454058876\n",
      "Sample IFCB1_2008_233_195822 (207/286) -> Accuracy: 0.8457551826258638\n",
      "Sample IFCB1_2008_114_171015 (208/286) -> Accuracy: 0.8630820399113082\n",
      "Sample IFCB1_2007_290_153737 (209/286) -> Accuracy: 0.8912184224093487\n",
      "Sample IFCB1_2007_283_000358 (210/286) -> Accuracy: 0.9348958333333334\n",
      "Sample IFCB1_2008_170_001609 (211/286) -> Accuracy: 0.7996755879967559\n",
      "Sample IFCB1_2008_349_174035 (212/286) -> Accuracy: 0.8747848537005164\n",
      "Sample IFCB1_2007_156_000402 (213/286) -> Accuracy: 0.9421487603305785\n",
      "Sample IFCB1_2008_100_190250 (214/286) -> Accuracy: 0.9617205998421468\n",
      "Sample IFCB1_2007_113_194743 (215/286) -> Accuracy: 0.9230769230769231\n",
      "Sample IFCB1_2007_290_155918 (216/286) -> Accuracy: 0.9108573376512699\n",
      "Sample IFCB1_2006_309_003838 (217/286) -> Accuracy: 0.7659717552118359\n",
      "Sample IFCB1_2006_280_004620 (218/286) -> Accuracy: 0.9502159581723119\n",
      "Sample IFCB1_2008_218_195118 (219/286) -> Accuracy: 0.8903483723586522\n",
      "Sample IFCB1_2008_073_144023 (220/286) -> Accuracy: 0.9530411449016101\n",
      "Sample IFCB1_2007_027_005040 (221/286) -> Accuracy: 0.9506065857885615\n",
      "Sample IFCB1_2007_024_213700 (222/286) -> Accuracy: 0.9371482176360225\n",
      "Sample IFCB1_2008_194_004923 (223/286) -> Accuracy: 0.9242264786360767\n",
      "Sample IFCB1_2006_280_000046 (224/286) -> Accuracy: 0.9310344827586207\n",
      "Sample IFCB1_2007_318_002657 (225/286) -> Accuracy: 0.9613470471134705\n",
      "Sample IFCB1_2007_010_001739 (226/286) -> Accuracy: 0.9045009784735812\n",
      "Sample IFCB1_2008_095_004102 (227/286) -> Accuracy: 0.9052868955149086\n",
      "Sample IFCB1_2006_361_004644 (228/286) -> Accuracy: 0.9399209486166008\n",
      "Sample IFCB1_2006_198_221339 (229/286) -> Accuracy: 0.9063039723661486\n",
      "Sample IFCB1_2006_270_170728 (230/286) -> Accuracy: 0.9057692307692308\n",
      "Sample IFCB1_2008_085_180755 (231/286) -> Accuracy: 0.9458621512043882\n",
      "Sample IFCB1_2008_116_211159 (232/286) -> Accuracy: 0.9224872231686542\n",
      "Sample IFCB1_2006_309_001656 (233/286) -> Accuracy: 0.7329545454545454\n",
      "Sample IFCB1_2006_171_005706 (234/286) -> Accuracy: 0.8322772277227722\n",
      "Sample IFCB1_2008_043_005824 (235/286) -> Accuracy: 0.9694052939154348\n",
      "Sample IFCB1_2008_218_001524 (236/286) -> Accuracy: 0.9408284023668639\n",
      "Sample IFCB1_2008_085_003117 (237/286) -> Accuracy: 0.9220799024092711\n",
      "Sample IFCB1_2007_318_004837 (238/286) -> Accuracy: 0.9571041030857917\n",
      "Sample IFCB1_2007_183_105701 (239/286) -> Accuracy: 0.8826563507414571\n",
      "Sample IFCB1_2008_233_202134 (240/286) -> Accuracy: 0.8516185096676081\n",
      "Sample IFCB1_2007_318_180338 (241/286) -> Accuracy: 0.9542743538767395\n",
      "Sample IFCB1_2008_233_204452 (242/286) -> Accuracy: 0.8614520490950697\n",
      "Sample IFCB1_2006_158_002223 (243/286) -> Accuracy: 0.8890737659456461\n",
      "Sample IFCB1_2007_146_000404 (244/286) -> Accuracy: 0.8927563499529633\n",
      "Sample IFCB1_2008_095_181849 (245/286) -> Accuracy: 0.8965217391304348\n",
      "Sample IFCB1_2006_158_000036 (246/286) -> Accuracy: 0.9113648722131593\n",
      "Sample IFCB1_2007_045_003217 (247/286) -> Accuracy: 0.8899297423887588\n",
      "Sample IFCB1_2007_101_184841 (248/286) -> Accuracy: 0.9364820846905537\n",
      "Sample IFCB1_2007_361_005810 (249/286) -> Accuracy: 0.8706099815157117\n",
      "Sample IFCB1_2006_280_004304 (250/286) -> Accuracy: 0.9388379204892966\n",
      "Sample IFCB1_2007_284_171849 (251/286) -> Accuracy: 0.9271758436944938\n",
      "Sample IFCB1_2008_006_004637 (252/286) -> Accuracy: 0.8557529610829103\n",
      "Sample IFCB1_2006_172_180317 (253/286) -> Accuracy: 0.8851933973885193\n",
      "Sample IFCB1_2008_194_000323 (254/286) -> Accuracy: 0.9215459384401758\n",
      "Sample IFCB1_2008_043_185649 (255/286) -> Accuracy: 0.9669302659956865\n",
      "Sample IFCB1_2007_045_013732 (256/286) -> Accuracy: 0.8826754385964912\n",
      "Sample IFCB1_2008_100_192454 (257/286) -> Accuracy: 0.9630943931866572\n",
      "Sample IFCB1_2007_129_004732 (258/286) -> Accuracy: 0.8904694167852063\n",
      "Sample IFCB1_2008_043_003620 (259/286) -> Accuracy: 0.9638386648122392\n",
      "Sample IFCB1_2007_058_034322 (260/286) -> Accuracy: 0.9322709163346613\n",
      "Sample IFCB1_2008_347_001931 (261/286) -> Accuracy: 0.9388539482879106\n",
      "Sample IFCB1_2008_043_181301 (262/286) -> Accuracy: 0.9587878787878787\n",
      "Sample IFCB1_2008_060_005303 (263/286) -> Accuracy: 0.8836314240434684\n",
      "Sample IFCB1_2007_032_181743 (264/286) -> Accuracy: 0.9071518193224593\n",
      "Sample IFCB1_2007_141_120042 (265/286) -> Accuracy: 0.8415119363395226\n",
      "Sample IFCB1_2007_304_004333 (266/286) -> Accuracy: 0.9026049444167911\n",
      "Sample IFCB1_2007_059_215122 (267/286) -> Accuracy: 0.9420289855072463\n",
      "Sample IFCB1_2006_307_215834 (268/286) -> Accuracy: 0.8202479338842975\n",
      "Sample IFCB1_2008_347_004306 (269/286) -> Accuracy: 0.9475587703435805\n",
      "Sample IFCB1_2007_303_202320 (270/286) -> Accuracy: 0.86875\n",
      "Sample IFCB1_2008_123_165550 (271/286) -> Accuracy: 0.875\n",
      "Sample IFCB1_2007_183_000344 (272/286) -> Accuracy: 0.8922670191672174\n",
      "Sample IFCB1_2007_326_004432 (273/286) -> Accuracy: 0.8397181858212242\n",
      "Sample IFCB1_2008_349_171852 (274/286) -> Accuracy: 0.8558182661103384\n",
      "Sample IFCB1_2007_089_004753 (275/286) -> Accuracy: 0.8645387310237447\n",
      "Sample IFCB1_2008_199_151912 (276/286) -> Accuracy: 0.9240216486261449\n",
      "Sample IFCB1_2007_129_184749 (277/286) -> Accuracy: 0.8652291105121294\n",
      "Sample IFCB1_2006_208_193127 (278/286) -> Accuracy: 0.8172992056487202\n",
      "Sample IFCB1_2008_072_001153 (279/286) -> Accuracy: 0.9406170752324599\n",
      "Sample IFCB1_2007_101_155445 (280/286) -> Accuracy: 0.8713692946058091\n",
      "Sample IFCB1_2007_114_000836 (281/286) -> Accuracy: 0.9252525252525252\n",
      "Sample IFCB1_2007_158_142523 (282/286) -> Accuracy: 0.9154302670623146\n",
      "Sample IFCB1_2008_200_001337 (283/286) -> Accuracy: 0.9106293972238068\n",
      "Sample IFCB1_2007_032_175539 (284/286) -> Accuracy: 0.9360400444938821\n",
      "Sample IFCB1_2008_199_154228 (285/286) -> Accuracy: 0.9158841940532081\n",
      "Sample IFCB1_2006_205_032616 (286/286) -> Accuracy: 0.8937275269676388\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Starting process...')\n",
    "\n",
    "model,loss_fn = load_network()\n",
    "\n",
    "if not is_trained:\n",
    "    finetune(model,loss_fn,train_loader,device)\n",
    "else:\n",
    "    print(\"Model was trained already\")\n",
    "\n",
    "print(\"Computing deep features for test images...\")\n",
    "path = os.path.join(results_save_path,'test')\n",
    "for index,sample in enumerate(samplestest):\n",
    "    file = [hdf5_files_path+sample+'.hdf5']\n",
    "    test_dset = H5IFCBDataset(file,classes,classattribute=classcolumn,verbose=0,trainingset=False,defaultclass='mix',transform=val_transform)\n",
    "    test_loader = DataLoader(test_dset,batch_size=batch_size_val,num_workers=num_workers)\n",
    "    y_true,y_pred,y_probs,y_logits,deepfeatures,_ = make_preds(model, test_loader, device)\n",
    "    print(\"Sample {} ({}/{}) -> Accuracy: {}\".format(sample,index+1,len(samplestest),accuracy_score(y_true,y_pred)))\n",
    "    np.savetxt(\"{}/{}_true.csv\".format(path,sample),y_true,fmt='%d')\n",
    "    np.savetxt(\"{}/{}_pred.csv\".format(path,sample),y_pred,fmt='%d')\n",
    "    np.savetxt(\"{}/{}_probs.csv\".format(path,sample), y_probs, delimiter=\",\",fmt='%1.8f')\n",
    "    np.savetxt(\"{}/{}_logits.csv\".format(path,sample), y_logits, delimiter=\",\",fmt='%f')\n",
    "    np.savetxt(\"{}/{}_deepfeatures.csv\".format(path,sample), deepfeatures, delimiter=\",\",fmt='%f')\n",
    "\n",
    "\n",
    "print(\"Predicting also train and val samples...\")\n",
    "path = os.path.join(results_save_path,'train')\n",
    "for index,sample in enumerate(samplestrainingval):\n",
    "    file = [hdf5_files_path+sample+'.hdf5']\n",
    "    train_dset = H5IFCBDataset(file,classes,classattribute=classcolumn,verbose=0,trainingset=False,defaultclass='mix',transform=val_transform)\n",
    "    train_loader = DataLoader(train_dset,batch_size=batch_size_val,num_workers=num_workers)\n",
    "    y_true,y_pred,y_probs,y_logits,deepfeatures,_ = make_preds(model, train_loader, device)\n",
    "    print(\"Sample {} ({}/{}) -> Accuracy: {}\".format(sample,index+1,len(samplestrainingval),accuracy_score(y_true,y_pred)))\n",
    "    np.savetxt(\"{}/{}_true.csv\".format(path,sample),y_true,fmt='%d')\n",
    "    np.savetxt(\"{}/{}_pred.csv\".format(path,sample),y_pred,fmt='%d')\n",
    "    np.savetxt(\"{}/{}_probs.csv\".format(path,sample), y_probs, delimiter=\",\",fmt='%1.8f')\n",
    "    np.savetxt(\"{}/{}_logits.csv\".format(path,sample), y_logits, delimiter=\",\",fmt='%f')\n",
    "    np.savetxt(\"{}/{}_deepfeatures.csv\".format(path,sample), deepfeatures, delimiter=\",\",fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the prevalences for the test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(results_save_path,'test')\n",
    "true_prevalences = pd.DataFrame(columns=classes,index=samplestest.sort(),dtype=float)\n",
    "for index,sample in enumerate(samplestest):\n",
    "    y_true = np.loadtxt(\"{}/{}_true.csv\".format(path,sample),dtype=int)\n",
    "    for i,cl in enumerate(classes):\n",
    "        true_prevalences.loc[sample,cl]=sum(y_true==i)\n",
    "    true_prevalences.loc[sample]=true_prevalences.loc[sample]/sum(true_prevalences.loc[sample])\n",
    "true_prevalences.to_csv(os.path.join(results_save_path,'test_prevalences.csv'),float_format='%.5f',index_label='sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format train files\n",
    "Each sample would be a file, with the label and then the features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [09:14<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "path = os.path.join(results_save_path, 'train')\n",
    "column_names = np.append('class', np.arange(512).astype(str))\n",
    "\n",
    "for sample in tqdm(samplestrainingval):\n",
    "    y = np.loadtxt(\"{}/{}_true.csv\".format(path, sample), dtype=int)\n",
    "    \n",
    "    deepfeatures = np.loadtxt(\"{}/{}_deepfeatures.csv\".format(path, sample), delimiter=',')\n",
    "\n",
    "    assert y.shape[0] == deepfeatures.shape[0], \"Should have the same dimension\"\n",
    "    assert deepfeatures.shape[1] == 512, \"Should have 512 features\"\n",
    "\n",
    "    # Create a single DataFrame for the entire sample\n",
    "    df = pd.DataFrame(np.hstack((classes[y.reshape(-1, 1)], deepfeatures)), columns=column_names)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df['class'] = df['class'].astype(str)\n",
    "    df.to_csv(os.path.join(path, f\"{sample}.csv\"), float_format='%.5f', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format test files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 678/678 [31:22<00:00,  2.78s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "path = os.path.join(results_save_path, 'test')\n",
    "column_names = np.arange(512).astype(str)\n",
    "\n",
    "for sample in tqdm(samplestest):\n",
    "    deepfeatures = np.loadtxt(\"{}/{}_deepfeatures.csv\".format(path, sample), delimiter=',')\n",
    "\n",
    "    assert deepfeatures.shape[1] == 512, \"Should have 512 features\"\n",
    "\n",
    "    # Create a single DataFrame for the entire sample\n",
    "    df = pd.DataFrame(deepfeatures, columns=column_names)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(os.path.join(path, f\"{sample}.csv\"), float_format='%.5f', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting train samples:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/286 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 286/286 [03:12<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting test samples:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 678/678 [09:27<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "os.mkdir('export')\n",
    "\n",
    "#test prevalences\n",
    "zip_file = zipfile.ZipFile('export/IFCB.test_prevalences.zip', mode='w')\n",
    "file_to_add = os.path.join(results_save_path, \"test_prevalences.csv\")\n",
    "filename = os.path.basename(file_to_add)\n",
    "zip_file.write(file_to_add, arcname=filename)\n",
    "zip_file.close()\n",
    "\n",
    "paths = [os.path.join(results_save_path,'train',s+'.csv') for s in samplestrainingval]\n",
    "print(\"Exporting train samples:\")\n",
    "with zipfile.ZipFile(os.path.join('export', 'IFCB.train.zip'), 'w') as zip_file:\n",
    "    for sample in tqdm(samplestrainingval):\n",
    "        file_path = os.path.join(results_save_path, 'train', sample + '.csv')\n",
    "        arcname = os.path.join('train',os.path.basename(file_path))\n",
    "        zip_file.write(file_path, arcname=arcname, compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_file.close()\n",
    "\n",
    "print(\"Exporting test samples:\")\n",
    "with zipfile.ZipFile(os.path.join('export', 'IFCB.test.zip'), 'w') as zip_file:\n",
    "    for sample in tqdm(samplestest):\n",
    "        file_path = os.path.join(results_save_path, 'test', sample + '.csv')\n",
    "        arcname = os.path.join('test',os.path.basename(file_path))\n",
    "        zip_file.write(file_path, arcname=arcname, compress_type=zipfile.ZIP_DEFLATED)\n",
    "zip_file.close()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNshTTuV/wHM/CMmYWttHqR",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "IFCB_FT_Baseline.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "4fba9110b1a4ec95baa236356b4366c963be065d6fde289654bed570a6fc51de"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
